{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cd793d5ba0be418e8e47231674a463b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8471b05ab2d24e748beec2dc4d1e8ba5",
              "IPY_MODEL_819bb4e3b09245648c89c92154af4c1c",
              "IPY_MODEL_be3b589eeac2471490128ec10405cf5d"
            ],
            "layout": "IPY_MODEL_7a3e7271091749e2ae6dd1064e6cb8c9"
          }
        },
        "8471b05ab2d24e748beec2dc4d1e8ba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_841d4f5844564072af1ead3adbc0f33a",
            "placeholder": "​",
            "style": "IPY_MODEL_dd612186a44b44a5a0d0dcd43fd9d348",
            "value": "Batches: 100%"
          }
        },
        "819bb4e3b09245648c89c92154af4c1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47e0efe5973345398398b91581991470",
            "max": 24,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74fd7a2c904048c0867f1349d3c7bf97",
            "value": 24
          }
        },
        "be3b589eeac2471490128ec10405cf5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f08400d702ae4f14834cd3b88432471d",
            "placeholder": "​",
            "style": "IPY_MODEL_9c2c248fa5b74555bcc50a2956ed37c9",
            "value": " 24/24 [00:39&lt;00:00,  1.75it/s]"
          }
        },
        "7a3e7271091749e2ae6dd1064e6cb8c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "841d4f5844564072af1ead3adbc0f33a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd612186a44b44a5a0d0dcd43fd9d348": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47e0efe5973345398398b91581991470": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74fd7a2c904048c0867f1349d3c7bf97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f08400d702ae4f14834cd3b88432471d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c2c248fa5b74555bcc50a2956ed37c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "X_Rlb_tWgWUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efe67845-51e6-4fd4-caf0-21026442bc7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.94.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.7)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install faiss-cpu python-dotenv openai sentence-transformers pdfplumber\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from dotenv import load_dotenv\n",
        "from openai import AzureOpenAI\n",
        "import pdfplumber\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "def init_client():\n",
        "    \"\"\"Initialize Azure OpenAI client with environment variables.\"\"\"\n",
        "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"AZURE_OPENAI_API_KEY environment variable not set.\")\n",
        "\n",
        "    endpoint = os.getenv(\"ENDPOINT_URL\")\n",
        "    if not endpoint:\n",
        "        raise ValueError(\"ENDPOINT_URL environment variable not set.\")\n",
        "\n",
        "    client = AzureOpenAI(\n",
        "        api_key=api_key,\n",
        "        api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
        "        azure_endpoint=endpoint\n",
        "    )\n",
        "    return client\n",
        "\n",
        "# Initialize client\n",
        "client = init_client()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_json_from_response(response: str) -> str:\n",
        "    \"\"\"Extract JSON from response, handling various formats\"\"\"\n",
        "    # Handle markdown code blocks\n",
        "    if '```json' in response:\n",
        "        match = re.search(r'```json\\s*\\n?(.*?)\\n?```', response, re.DOTALL)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "\n",
        "    if '```' in response:\n",
        "        match = re.search(r'```\\s*\\n?(.*?)\\n?```', response, re.DOTALL)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "\n",
        "    # Find JSON objects\n",
        "    json_pattern = r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}'\n",
        "    matches = re.findall(json_pattern, response, re.DOTALL)\n",
        "    if matches:\n",
        "        return max(matches, key=len).strip()\n",
        "\n",
        "    return response.strip()\n",
        "\n",
        "def load_pdf_with_advanced_chunking(pdf_dir: str, pdf_files: List[str]) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Load PDFs with advanced chunking strategies for better retrieval.\n",
        "    Returns list of chunks with metadata.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        file_path = os.path.join(pdf_dir, pdf_file)\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Warning: {pdf_file} not found.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with pdfplumber.open(file_path) as pdf:\n",
        "                for page_num, page in enumerate(pdf.pages):\n",
        "                    text = page.extract_text() or \"\"\n",
        "                    if not text.strip():\n",
        "                        continue\n",
        "\n",
        "                    # Advanced chunking strategies\n",
        "                    page_chunks = create_advanced_chunks(text, page_num, pdf_file)\n",
        "                    chunks.extend(page_chunks)\n",
        "\n",
        "            print(f\"Extracted {len([c for c in chunks if c['source'] == pdf_file])} chunks from {pdf_file}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {pdf_file}: {str(e)}\")\n",
        "\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "8icjcfz2gfkV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_advanced_chunks(text: str, page_num: int, source: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Create advanced chunks using multiple strategies:\n",
        "    1. Sentence-based chunking\n",
        "    2. Paragraph-based chunking\n",
        "    3. Overlapping chunks\n",
        "    4. Semantic chunks based on headers/sections\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    # Clean text\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Strategy 1: Sentence-based chunking\n",
        "    sentences = sent_tokenize(text)\n",
        "    sentence_chunks = create_sentence_chunks(sentences, chunk_size=3, overlap=1)\n",
        "\n",
        "    for i, chunk in enumerate(sentence_chunks):\n",
        "        chunks.append({\n",
        "            'text': chunk,\n",
        "            'source': source,\n",
        "            'page': page_num,\n",
        "            'chunk_id': f\"{source}_p{page_num}_sent_{i}\",\n",
        "            'chunk_type': 'sentence',\n",
        "            'length': len(chunk)\n",
        "        })\n",
        "\n",
        "    # Strategy 2: Paragraph-based chunking\n",
        "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "    paragraph_chunks = create_paragraph_chunks(paragraphs, max_length=800, overlap=100)\n",
        "\n",
        "    for i, chunk in enumerate(paragraph_chunks):\n",
        "        chunks.append({\n",
        "            'text': chunk,\n",
        "            'source': source,\n",
        "            'page': page_num,\n",
        "            'chunk_id': f\"{source}_p{page_num}_para_{i}\",\n",
        "            'chunk_type': 'paragraph',\n",
        "            'length': len(chunk)\n",
        "        })\n",
        "\n",
        "    # Strategy 3: Semantic chunking (based on headers/sections)\n",
        "    semantic_chunks = create_semantic_chunks(text)\n",
        "\n",
        "    for i, chunk in enumerate(semantic_chunks):\n",
        "        chunks.append({\n",
        "            'text': chunk,\n",
        "            'source': source,\n",
        "            'page': page_num,\n",
        "            'chunk_id': f\"{source}_p{page_num}_semantic_{i}\",\n",
        "            'chunk_type': 'semantic',\n",
        "            'length': len(chunk)\n",
        "        })\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "06mB6zjpGssN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sentence_chunks(sentences: List[str], chunk_size: int = 3, overlap: int = 1) -> List[str]:\n",
        "    \"\"\"Create overlapping sentence chunks\"\"\"\n",
        "    chunks = []\n",
        "    for i in range(0, len(sentences), chunk_size - overlap):\n",
        "        chunk_sentences = sentences[i:i + chunk_size]\n",
        "        if chunk_sentences:\n",
        "            chunks.append(' '.join(chunk_sentences))\n",
        "    return chunks\n",
        "\n",
        "def create_paragraph_chunks(paragraphs: List[str], max_length: int = 800, overlap: int = 100) -> List[str]:\n",
        "    \"\"\"Create overlapping paragraph chunks\"\"\"\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for para in paragraphs:\n",
        "        if len(current_chunk) + len(para) <= max_length:\n",
        "            current_chunk += para + \"\\n\\n\"\n",
        "        else:\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "                # Create overlap\n",
        "                current_chunk = current_chunk[-overlap:] + para + \"\\n\\n\"\n",
        "            else:\n",
        "                current_chunk = para + \"\\n\\n\"\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def create_semantic_chunks(text: str) -> List[str]:\n",
        "    \"\"\"Create chunks based on semantic sections (headers, numbered sections, etc.)\"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    # Pattern for detecting sections/headers\n",
        "    section_pattern = r'(?:^|\\n)(?:\\d+\\.?\\s+|[A-Z][^.]*:|\\n[A-Z][A-Z\\s]+\\n|Chapter\\s+\\d+|Section\\s+\\d+)'\n",
        "\n",
        "    sections = re.split(section_pattern, text)\n",
        "\n",
        "    for section in sections:\n",
        "        section = section.strip()\n",
        "        if len(section) > 100:  # Only keep substantial sections\n",
        "            chunks.append(section)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def build_enhanced_vector_index(chunks: List[Dict]) -> Tuple[faiss.IndexFlatL2, SentenceTransformer, List[Dict]]:\n",
        "    \"\"\"Build enhanced vector index with multiple embedding strategies\"\"\"\n",
        "\n",
        "    # Initialize embedder\n",
        "    embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    # Extract texts for embedding\n",
        "    texts = [chunk['text'] for chunk in chunks]\n",
        "\n",
        "    # Create embeddings\n",
        "    embeddings = embedder.encode(texts, convert_to_tensor=False, show_progress_bar=True)\n",
        "\n",
        "    # Create FAISS index\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(np.array(embeddings, dtype=np.float32))\n",
        "\n",
        "    print(f\"Built vector index with {len(chunks)} chunks\")\n",
        "\n",
        "    return index, embedder, chunks\n",
        "\n",
        "def retrieve_enhanced_context(query: str, index: faiss.IndexFlatL2, embedder: SentenceTransformer,\n",
        "                            chunks: List[Dict], k: int = 8) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Enhanced context retrieval with multiple strategies:\n",
        "    1. Semantic similarity search\n",
        "    2. Keyword matching\n",
        "    3. Diversity-based reranking\n",
        "    \"\"\"\n",
        "\n",
        "    # Semantic search\n",
        "    query_embedding = embedder.encode([query], convert_to_tensor=False)\n",
        "    distances, indices = index.search(np.array(query_embedding, dtype=np.float32), k * 2)\n",
        "\n",
        "    # Get candidate chunks\n",
        "    candidate_chunks = []\n",
        "    for i in indices[0]:\n",
        "        if i < len(chunks):\n",
        "            chunk = chunks[i].copy()\n",
        "            chunk['similarity_score'] = float(1 / (1 + distances[0][len(candidate_chunks)]))\n",
        "            candidate_chunks.append(chunk)\n",
        "\n",
        "    # Keyword matching boost\n",
        "    query_keywords = set(query.lower().split())\n",
        "    for chunk in candidate_chunks:\n",
        "        chunk_keywords = set(chunk['text'].lower().split())\n",
        "        keyword_overlap = len(query_keywords.intersection(chunk_keywords))\n",
        "        chunk['keyword_score'] = keyword_overlap / len(query_keywords) if query_keywords else 0\n",
        "\n",
        "    # Diversity-based reranking\n",
        "    final_chunks = rerank_for_diversity(candidate_chunks, k)\n",
        "\n",
        "    return final_chunks"
      ],
      "metadata": {
        "id": "B9kvu7fsG7Cm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rerank_for_diversity(chunks: List[Dict], k: int) -> List[Dict]:\n",
        "    \"\"\"Rerank chunks to ensure diversity in sources and content\"\"\"\n",
        "\n",
        "    # Sort by combined score (similarity + keyword)\n",
        "    for chunk in chunks:\n",
        "        chunk['combined_score'] = chunk['similarity_score'] + chunk['keyword_score'] * 0.3\n",
        "\n",
        "    chunks.sort(key=lambda x: x['combined_score'], reverse=True)\n",
        "\n",
        "    # Ensure diversity\n",
        "    selected_chunks = []\n",
        "    used_sources = set()\n",
        "    used_chunk_types = set()\n",
        "\n",
        "    for chunk in chunks:\n",
        "        if len(selected_chunks) >= k:\n",
        "            break\n",
        "\n",
        "        # Prioritize diversity in sources and chunk types\n",
        "        source_penalty = 0.1 if chunk['source'] in used_sources else 0\n",
        "        type_penalty = 0.05 if chunk['chunk_type'] in used_chunk_types else 0\n",
        "\n",
        "        chunk['final_score'] = chunk['combined_score'] - source_penalty - type_penalty\n",
        "\n",
        "        selected_chunks.append(chunk)\n",
        "        used_sources.add(chunk['source'])\n",
        "        used_chunk_types.add(chunk['chunk_type'])\n",
        "\n",
        "    return selected_chunks\n",
        "\n",
        "# Simple retrieve context function from openai_llm_as_a_judge\n",
        "def retrieve_simple_context(query: str, embedder: SentenceTransformer, chunks: List[Dict], k: int = 3) -> List[str]:\n",
        "    \"\"\"Simple context retrieval function from openai_llm_as_a_judge\"\"\"\n",
        "    sentences = [chunk['text'] for chunk in chunks]\n",
        "    if not sentences:\n",
        "        print(\"Warning: No valid sentences extracted from chunks.\")\n",
        "        return [\"No relevant context found.\"]\n",
        "\n",
        "    embeddings = embedder.encode(sentences, convert_to_numpy=True)\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    index.add(embeddings)\n",
        "\n",
        "    query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
        "    distances, indices = index.search(query_embedding, k=k)\n",
        "\n",
        "    retrieved_context = [sentences[i] for i in indices[0] if i < len(sentences)]\n",
        "    print(f\"Query: {query}, Sentences Count: {len(sentences)}, Retrieved Context: {retrieved_context[:100]}...\")\n",
        "\n",
        "    return retrieved_context if retrieved_context else [\"No relevant context found.\"]\n",
        "\n",
        "def generate_response_with_rag(question: str, retrieved_chunks: List[Dict]) -> str:\n",
        "    \"\"\"Generate response using retrieved context (from llm_as_a_judge)\"\"\"\n",
        "\n",
        "    # Combine context from chunks\n",
        "    context_parts = []\n",
        "    for chunk in retrieved_chunks:\n",
        "        context_parts.append(f\"[{chunk['source']} - Page {chunk['page']}]: {chunk['text']}\")\n",
        "\n",
        "    context = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "    prompt = f\"\"\"Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Based on the provided context, please answer the question accurately and concisely. If the context doesn't contain enough information to answer the question, please state that clearly.\"\"\"\n",
        "\n",
        "    deployment = os.getenv(\"DEPLOYMENT_NAME\")\n",
        "    if not deployment:\n",
        "        raise ValueError(\"DEPLOYMENT_NAME is missing in .env\")\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=deployment,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant that answers questions based on provided context from insurance policy documents.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=200,\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def generate_simple_response(query: str) -> str:\n",
        "    \"\"\"Generate response without RAG (from openai_llm_as_a_judge)\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=os.getenv(\"DEPLOYMENT_NAME\"),\n",
        "        messages=[{\"role\": \"user\", \"content\": query}],\n",
        "        max_tokens=50,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()"
      ],
      "metadata": {
        "id": "Ybigu_TVHEjf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def judge_response_quality(question: str, response: str, ground_truth: str, context: str,\n",
        "                         use_ground_truth: bool = True) -> Dict[str, any]:\n",
        "    \"\"\"\n",
        "    Evaluate response quality using multiple criteria (from llm_as_a_judge)\n",
        "    \"\"\"\n",
        "\n",
        "    if use_ground_truth:\n",
        "        evaluation_prompt = f\"\"\"You are an expert evaluator. Please evaluate the following response across multiple dimensions:\n",
        "\n",
        "Question: {question}\n",
        "Context Provided: {context}\n",
        "Generated Response: {response}\n",
        "Ground Truth Answer: {ground_truth}\n",
        "\n",
        "Please evaluate on these criteria (each on a scale of 0-1):\n",
        "1. **Accuracy**: How factually correct is the response compared to the ground truth?\n",
        "2. **Completeness**: Does the response cover all important points from the ground truth?\n",
        "3. **Relevance**: Is the response relevant to the question and uses the context appropriately?\n",
        "4. **Conciseness**: Is the response clear and concise without unnecessary information?\n",
        "\n",
        "Provide your evaluation in the following JSON format:\n",
        "{{\n",
        "    \"accuracy\": <float>,\n",
        "    \"completeness\": <float>,\n",
        "    \"relevance\": <float>,\n",
        "    \"conciseness\": <float>,\n",
        "    \"overall_score\": <float>,\n",
        "    \"explanation\": \"<brief explanation of the scores>\"\n",
        "}}\"\"\"\n",
        "    else:\n",
        "        evaluation_prompt = f\"\"\"You are an expert evaluator. Please evaluate the following response across multiple dimensions:\n",
        "\n",
        "Question: {question}\n",
        "Context Provided: {context}\n",
        "Generated Response: {response}\n",
        "\n",
        "Please evaluate on these criteria (each on a scale of 0-1):\n",
        "1. **Accuracy**: How factually correct is the response based on the context?\n",
        "2. **Completeness**: Does the response adequately address the question?\n",
        "3. **Relevance**: Is the response relevant to the question and uses the context appropriately?\n",
        "4. **Conciseness**: Is the response clear and concise without unnecessary information?\n",
        "\n",
        "Provide your evaluation in the following JSON format:\n",
        "{{\n",
        "    \"accuracy\": <float>,\n",
        "    \"completeness\": <float>,\n",
        "    \"relevance\": <float>,\n",
        "    \"conciseness\": <float>,\n",
        "    \"overall_score\": <float>,\n",
        "    \"explanation\": \"<brief explanation of the scores>\"\n",
        "}}\"\"\"\n",
        "\n",
        "    deployment = os.getenv(\"DEPLOYMENT_NAME\")\n",
        "    completion = client.chat.completions.create(\n",
        "        model=deployment,\n",
        "        messages=[{\"role\": \"user\", \"content\": evaluation_prompt}],\n",
        "        max_tokens=300,\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    result = completion.choices[0].message.content.strip()\n",
        "\n",
        "    try:\n",
        "        json_content = extract_json_from_response(result)\n",
        "        evaluation = json.loads(json_content)\n",
        "\n",
        "        # Ensure all required fields exist\n",
        "        required_fields = ['accuracy', 'completeness', 'relevance', 'conciseness', 'overall_score', 'explanation']\n",
        "        for field in required_fields:\n",
        "            if field not in evaluation:\n",
        "                evaluation[field] = 0.0 if field != 'explanation' else 'Field missing from evaluation'\n",
        "\n",
        "        return evaluation\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Failed to parse JSON response: {result}\")\n",
        "        return {\n",
        "            \"accuracy\": 0.0,\n",
        "            \"completeness\": 0.0,\n",
        "            \"relevance\": 0.0,\n",
        "            \"conciseness\": 0.0,\n",
        "            \"overall_score\": 0.0,\n",
        "            \"explanation\": \"Failed to parse evaluation response\"\n",
        "        }"
      ],
      "metadata": {
        "id": "JjSmavlGHJ_J"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def judge_context_relevance(question: str, retrieved_chunks: List[Dict]) -> Dict[str, any]:\n",
        "    \"\"\"\n",
        "    Evaluate how relevant the retrieved context is to the question (from llm_as_a_judge)\n",
        "    \"\"\"\n",
        "\n",
        "    # Combine context from chunks\n",
        "    context_parts = []\n",
        "    for chunk in retrieved_chunks:\n",
        "        context_parts.append(f\"[{chunk['source']}]: {chunk['text'][:200]}...\")\n",
        "\n",
        "    context = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "    prompt = f\"\"\"You are an expert evaluator. Please evaluate how relevant the following context is to the question.\n",
        "\n",
        "Question: {question}\n",
        "Retrieved Context: {context}\n",
        "\n",
        "Please evaluate:\n",
        "1. **Relevance Score** (0-1): How relevant is the context to answering the question?\n",
        "2. **Coverage Score** (0-1): How well does the context cover the information needed to answer the question?\n",
        "3. **Noise Level** (0-1): How much irrelevant information is in the context? (0 = lots of noise, 1 = no noise)\n",
        "\n",
        "Provide your evaluation in the following JSON format:\n",
        "{{\n",
        "    \"relevance_score\": <float>,\n",
        "    \"coverage_score\": <float>,\n",
        "    \"noise_level\": <float>,\n",
        "    \"explanation\": \"<brief explanation>\"\n",
        "}}\"\"\"\n",
        "\n",
        "    deployment = os.getenv(\"DEPLOYMENT_NAME\")\n",
        "    completion = client.chat.completions.create(\n",
        "        model=deployment,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=200,\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    result = completion.choices[0].message.content.strip()\n",
        "\n",
        "    try:\n",
        "        json_content = extract_json_from_response(result)\n",
        "        evaluation = json.loads(json_content)\n",
        "\n",
        "        # Ensure all required fields exist\n",
        "        required_fields = ['relevance_score', 'coverage_score', 'noise_level', 'explanation']\n",
        "        for field in required_fields:\n",
        "            if field not in evaluation:\n",
        "                evaluation[field] = 0.0 if field != 'explanation' else 'Field missing from evaluation'\n",
        "\n",
        "        return evaluation\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Failed to parse JSON response: {result}\")\n",
        "        return {\n",
        "            \"relevance_score\": 0.0,\n",
        "            \"coverage_score\": 0.0,\n",
        "            \"noise_level\": 0.0,\n",
        "            \"explanation\": \"Failed to parse evaluation response\"\n",
        "        }"
      ],
      "metadata": {
        "id": "Uzp8EiwoHRDR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def judge_retriever(query: str, retrieved_context: List[str]) -> Tuple[float, str]:\n",
        "    \"\"\"Evaluate the relevance of retrieved context to the query (from openai_llm_as_a_judge)\"\"\"\n",
        "    context_text = \"\\n\".join(retrieved_context)\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert evaluator. Assess the relevance of the following retrieved context to the query.\n",
        "    Query: {query}\n",
        "    Retrieved Context: {context_text}\n",
        "    Provide a relevance score from 0 to 1 (0 = completely irrelevant, 1 = perfectly relevant) and a brief explanation.\n",
        "    Format your response exactly as: 'Score: <number>\\nExplanation: <text>' where <number> is a float (e.g., 0.9) and <text> is the explanation.\n",
        "    \"\"\"\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=os.getenv(\"DEPLOYMENT_NAME\"),\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=150,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    result = completion.choices[0].message.content.strip()\n",
        "\n",
        "    # Parse score and explanation\n",
        "    try:\n",
        "        score_line = next((line for line in result.split(\"\\n\") if line.startswith(\"Score:\")), \"\").strip()\n",
        "        explanation_line = next((line for line in result.split(\"\\n\") if line.startswith(\"Explanation:\")), \"\").strip()\n",
        "\n",
        "        if score_line:\n",
        "            score = float(score_line.replace(\"Score:\", \"\").strip())\n",
        "        else:\n",
        "            score = 0.0\n",
        "\n",
        "        if explanation_line:\n",
        "            explanation = explanation_line.replace(\"Explanation:\", \"\").strip()\n",
        "        else:\n",
        "            explanation = \"No explanation provided\"\n",
        "    except (ValueError, StopIteration) as e:\n",
        "        score = 0.0\n",
        "        explanation = f\"Failed to parse retriever response. Error: {str(e)}\"\n",
        "\n",
        "    return score, explanation\n"
      ],
      "metadata": {
        "id": "ycEyURlDHR01"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_qa_dataset(file_path: str) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"Load questions and ground truths from Excel dataset\"\"\"\n",
        "\n",
        "    if file_path.endswith('.xlsx'):\n",
        "        df = pd.read_excel(file_path, header=None)\n",
        "    elif file_path.endswith('.csv'):\n",
        "        df = pd.read_csv(file_path, header=None)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Use Excel (.xlsx) or CSV (.csv)\")\n",
        "\n",
        "    # Find the row with \"question\" to locate the start of data\n",
        "    start_row = df[df[0].str.contains(\"question\", na=False)].index[0]\n",
        "    data = df.iloc[start_row + 1:].dropna(how='all')\n",
        "\n",
        "    print(\"Raw data preview:\", data.head())\n",
        "\n",
        "    questions = data[0].dropna().tolist()\n",
        "    ground_truths = data[1].dropna().tolist()\n",
        "\n",
        "    return questions, ground_truths\n",
        "\n",
        "def run_enhanced_evaluation(questions: List[str], ground_truths: List[str],\n",
        "                          index: faiss.IndexFlatL2, embedder: SentenceTransformer,\n",
        "                          chunks: List[Dict], use_ground_truth: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Run comprehensive RAG evaluation with enhanced metrics\n",
        "    \"\"\"\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i, (question, ground_truth) in enumerate(zip(questions, ground_truths)):\n",
        "        print(f\"\\nProcessing {i + 1}/{len(questions)}: {question[:50]}...\")\n",
        "\n",
        "        # 1. Retrieve context using enhanced retrieval\n",
        "        retrieved_chunks = retrieve_enhanced_context(question, index, embedder, chunks, k=5)\n",
        "\n",
        "        # 2. Also get simple context for comparison\n",
        "        simple_context = retrieve_simple_context(question, embedder, chunks, k=3)\n",
        "\n",
        "        # 3. Evaluate context relevance\n",
        "        context_eval = judge_context_relevance(question, retrieved_chunks)\n",
        "\n",
        "        # 4. Evaluate simple retriever (from openai_llm_as_a_judge style)\n",
        "        retriever_score, retriever_explanation = judge_retriever(question, simple_context)\n",
        "\n",
        "        # 5. Generate response with enhanced RAG\n",
        "        generated_response = generate_response_with_rag(question, retrieved_chunks)\n",
        "\n",
        "        # 6. Evaluate response quality\n",
        "        context_text = \"\\n\\n\".join([f\"[{chunk['source']}]: {chunk['text']}\" for chunk in retrieved_chunks])\n",
        "        response_eval = judge_response_quality(question, generated_response, ground_truth,\n",
        "                                             context_text, use_ground_truth)\n",
        "\n",
        "        # Compile results\n",
        "        result = {\n",
        "            'question': question,\n",
        "            'ground_truth': ground_truth,\n",
        "            'generated_response': generated_response,\n",
        "            'retrieved_chunks_count': len(retrieved_chunks),\n",
        "            'context_sources': list(set([chunk['source'] for chunk in retrieved_chunks])),\n",
        "\n",
        "            # Enhanced context evaluation metrics\n",
        "            'context_relevance': context_eval.get('relevance_score', 0.0),\n",
        "            'context_coverage': context_eval.get('coverage_score', 0.0),\n",
        "            'context_noise_level': context_eval.get('noise_level', 0.0),\n",
        "            'context_explanation': context_eval.get('explanation', 'N/A'),\n",
        "\n",
        "            # Simple retriever evaluation (openai style)\n",
        "            'simple_retriever_score': retriever_score,\n",
        "            'simple_retriever_explanation': retriever_explanation,\n",
        "\n",
        "            # Response evaluation metrics\n",
        "            'response_accuracy': response_eval.get('accuracy', 0.0),\n",
        "            'response_completeness': response_eval.get('completeness', 0.0),\n",
        "            'response_relevance': response_eval.get('relevance', 0.0),\n",
        "            'response_conciseness': response_eval.get('conciseness', 0.0),\n",
        "            'response_overall_score': response_eval.get('overall_score', 0.0),\n",
        "            'response_explanation': response_eval.get('explanation', 'N/A'),\n",
        "\n",
        "            # Chunk details\n",
        "            'chunk_details': [{\n",
        "                'source': chunk['source'],\n",
        "                'chunk_type': chunk['chunk_type'],\n",
        "                'similarity_score': chunk.get('similarity_score', 0.0),\n",
        "                'text_preview': chunk['text'][:100] + '...' if len(chunk['text']) > 100 else chunk['text']\n",
        "            } for chunk in retrieved_chunks]\n",
        "        }\n",
        "\n",
        "        results.append(result)\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Context Relevance: {context_eval['relevance_score']:.2f}\")\n",
        "        print(f\"Simple Retriever Score: {retriever_score:.2f}\")\n",
        "        print(f\"Response Overall: {response_eval['overall_score']:.2f}\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def calculate_evaluation_metrics(results_df: pd.DataFrame) -> Dict[str, float]:\n",
        "    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
        "\n",
        "    metrics = {\n",
        "        # Context metrics\n",
        "        'avg_context_relevance': results_df['context_relevance'].mean(),\n",
        "        'avg_context_coverage': results_df['context_coverage'].mean(),\n",
        "        'avg_context_noise_level': results_df['context_noise_level'].mean(),\n",
        "        'avg_simple_retriever_score': results_df['simple_retriever_score'].mean(),\n",
        "\n",
        "        # Response metrics\n",
        "        'avg_response_accuracy': results_df['response_accuracy'].mean(),\n",
        "        'avg_response_completeness': results_df['response_completeness'].mean(),\n",
        "        'avg_response_relevance': results_df['response_relevance'].mean(),\n",
        "        'avg_response_conciseness': results_df['response_conciseness'].mean(),\n",
        "        'avg_response_overall': results_df['response_overall_score'].mean(),\n",
        "\n",
        "        # Standard deviations\n",
        "        'std_context_relevance': results_df['context_relevance'].std(),\n",
        "        'std_response_overall': results_df['response_overall_score'].std(),\n",
        "\n",
        "        # Performance distribution\n",
        "        'excellent_responses': (results_df['response_overall_score'] >= 0.8).sum() / len(results_df),\n",
        "        'good_responses': ((results_df['response_overall_score'] >= 0.6) &\n",
        "                          (results_df['response_overall_score'] < 0.8)).sum() / len(results_df),\n",
        "        'poor_responses': (results_df['response_overall_score'] < 0.6).sum() / len(results_df),\n",
        "\n",
        "        # Retrieval metrics\n",
        "        'avg_chunks_retrieved': results_df['retrieved_chunks_count'].mean(),\n",
        "        'avg_sources_used': results_df['context_sources'].apply(len).mean(),\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def generate_evaluation_report(results_df: pd.DataFrame, metrics: Dict[str, float],\n",
        "                             output_dir: str = '.') -> None:\n",
        "    \"\"\"Generate comprehensive evaluation report\"\"\"\n",
        "\n",
        "    import os\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Save detailed results\n",
        "    results_df.to_csv(f\"{output_dir}/merged_rag_evaluation_detailed.csv\", index=False)\n",
        "\n",
        "    # Create summary report\n",
        "    report = f\"\"\"\n",
        "# Merged Enhanced RAG Evaluation Report\n",
        "\n",
        "## Dataset Summary\n",
        "- Total samples evaluated: {len(results_df)}\n",
        "- Evaluation date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "- Average chunks retrieved per query: {metrics['avg_chunks_retrieved']:.1f}\n",
        "- Average sources used per query: {metrics['avg_sources_used']:.1f}\n",
        "\n",
        "## Context Retrieval Quality\n",
        "- Average Relevance: {metrics['avg_context_relevance']:.3f} (±{metrics['std_context_relevance']:.3f})\n",
        "- Average Coverage: {metrics['avg_context_coverage']:.3f}\n",
        "- Average Noise Level: {metrics['avg_context_noise_level']:.3f}\n",
        "- Simple Retriever Score: {metrics['avg_simple_retriever_score']:.3f}\n",
        "\n",
        "## Response Generation Quality\n",
        "- Average Accuracy: {metrics['avg_response_accuracy']:.3f}\n",
        "- Average Completeness: {metrics['avg_response_completeness']:.3f}\n",
        "- Average Relevance: {metrics['avg_response_relevance']:.3f}\n",
        "- Average Conciseness: {metrics['avg_response_conciseness']:.3f}\n",
        "- **Overall Score: {metrics['avg_response_overall']:.3f} (±{metrics['std_response_overall']:.3f})**\n",
        "\n",
        "## Performance Distribution\n",
        "- Excellent (≥0.8): {metrics['excellent_responses']:.1%}\n",
        "- Good (0.6-0.8): {metrics['good_responses']:.1%}\n",
        "- Poor (<0.6): {metrics['poor_responses']:.1%}\n",
        "\n",
        "## Top Performing Examples\n",
        "{results_df.nlargest(3, 'response_overall_score')[['question', 'response_overall_score', 'response_explanation']].to_string()}\n",
        "\n",
        "## Worst Performing Examples\n",
        "{results_df.nsmallest(3, 'response_overall_score')[['question', 'response_overall_score', 'response_explanation']].to_string()}\n",
        "\n",
        "## Retrieval Analysis\n",
        "- Context relevance vs Response quality correlation: {results_df['context_relevance'].corr(results_df['response_overall_score']):.3f}\n",
        "- Simple retriever vs Enhanced retriever correlation: {results_df['simple_retriever_score'].corr(results_df['context_relevance']):.3f}\n",
        "- Sources most frequently used: {results_df.explode('context_sources')['context_sources'].value_counts().head().to_dict()}\n",
        "\"\"\"\n",
        "\n",
        "    # Save report\n",
        "    with open(f\"{output_dir}/merged_rag_evaluation_report.txt\", 'w') as f:\n",
        "        f.write(report)\n",
        "\n",
        "    # Save metrics as JSON\n",
        "    with open(f\"{output_dir}/merged_rag_metrics.json\", 'w') as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "\n",
        "    print(report)\n",
        "    print(f\"\\nResults saved to {output_dir}/\")\n",
        "\n",
        "# Main execution function\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    # Configuration\n",
        "    pdf_dir = \"/content/\"\n",
        "    pdf_files = [\"indoor_asr.pdf\"]  # or [\"aegon_policy.pdf\", \"asr_policy.pdf\"]\n",
        "    qa_file = \"asr_indoor2.xlsx\"  # or \"Groundtruth inboedel translated.xlsx\"\n",
        "    output_dir = \"merged_evaluation_results\"\n",
        "\n",
        "    # Whether to use ground truth in evaluation\n",
        "    use_ground_truth = False  # Set to False to evaluate without ground truth\n",
        "\n",
        "    try:\n",
        "        print(\"=== Merged Enhanced RAG Evaluation System ===\")\n",
        "\n",
        "        # 1. Load and chunk documents\n",
        "        print(\"\\n1. Loading and chunking documents...\")\n",
        "        chunks = load_pdf_with_advanced_chunking(pdf_dir, pdf_files)\n",
        "        print(f\"Created {len(chunks)} chunks total\")\n",
        "\n",
        "        # 2. Build vector index\n",
        "        print(\"\\n2. Building enhanced vector index...\")\n",
        "        index, embedder, chunks = build_enhanced_vector_index(chunks)\n",
        "\n",
        "        # 3. Load QA dataset\n",
        "        print(\"\\n3. Loading QA dataset...\")\n",
        "        questions, ground_truths = load_qa_dataset(qa_file)\n",
        "        print(f\"Loaded {len(questions)} questions\")\n",
        "\n",
        "        # 4. Run evaluation\n",
        "        print(\"\\n4. Running merged enhanced evaluation...\")\n",
        "        results_df = run_enhanced_evaluation(questions, ground_truths, index, embedder, chunks, use_ground_truth)\n",
        "\n",
        "        # 5. Calculate metrics\n",
        "        print(\"\\n5. Calculating evaluation metrics...\")\n",
        "        metrics = calculate_evaluation_metrics(results_df)\n",
        "\n",
        "        # 6. Generate report\n",
        "        print(\"\\n6. Generating evaluation report...\")\n",
        "        generate_evaluation_report(results_df, metrics, output_dir)\n",
        "\n",
        "        print(\"\\n=== Evaluation Complete ===\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during evaluation: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cd793d5ba0be418e8e47231674a463b4",
            "8471b05ab2d24e748beec2dc4d1e8ba5",
            "819bb4e3b09245648c89c92154af4c1c",
            "be3b589eeac2471490128ec10405cf5d",
            "7a3e7271091749e2ae6dd1064e6cb8c9",
            "841d4f5844564072af1ead3adbc0f33a",
            "dd612186a44b44a5a0d0dcd43fd9d348",
            "47e0efe5973345398398b91581991470",
            "74fd7a2c904048c0867f1349d3c7bf97",
            "f08400d702ae4f14834cd3b88432471d",
            "9c2c248fa5b74555bcc50a2956ed37c9"
          ]
        },
        "id": "BW3L_pW5HX6d",
        "outputId": "0cb024fa-3e15-4d03-abc0-e540fcd9d6db"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Merged Enhanced RAG Evaluation System ===\n",
            "\n",
            "1. Loading and chunking documents...\n",
            "Extracted 746 chunks from indoor_asr.pdf\n",
            "Created 746 chunks total\n",
            "\n",
            "2. Building enhanced vector index...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/24 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd793d5ba0be418e8e47231674a463b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built vector index with 746 chunks\n",
            "\n",
            "3. Loading QA dataset...\n",
            "Raw data preview:                                                    0  \\\n",
            "1  A ball went through my window. Will this be co...   \n",
            "2               I dropped my phone. Is this covered?   \n",
            "3  My roof is leaking, causing damage to the ceil...   \n",
            "4  I tripped outside and broke my glasses. Is thi...   \n",
            "5  My phone slipped out of my hands and fell into...   \n",
            "\n",
            "                                                   1  \n",
            "1  Yes, glass breakage due to sudden and unforese...  \n",
            "2  Yes, damage to your phone due to sudden and un...  \n",
            "3  No, painting the ceiling due to water damage f...  \n",
            "4  No, damage to your glasses due to tripping out...  \n",
            "5  No, this is not insured. Loss of your mobile e...  \n",
            "Loaded 12 questions\n",
            "\n",
            "4. Running merged enhanced evaluation...\n",
            "\n",
            "Processing 1/12: A ball went through my window. Will this be covere...\n",
            "Query: A ball went through my window. Will this be covered?, Sentences Count: 746, Retrieved Context: ['Then you will receive compensation in money. Do you have optional Glass coverage insured? Then this also applies in case of broken windows.', 'Do you have optional Glass coverage insured? Then this also applies in case of broken windows. Selected repair company We work with damage repair companies that we have selected.', 'NB: If your policy does not state that Glass is insured, then the glass in your home is not insured. But for events according to the basic coverage as mentioned in 3.1 Event Coverage Limitation Broken windows residential homeYou are insured for the replacement of your broken windows by glass of the same type and quality. We also reimburse the necessary painting work on the window of the broken window.']...\n",
            "Context Relevance: 0.90\n",
            "Simple Retriever Score: 0.90\n",
            "Response Overall: 1.00\n",
            "\n",
            "Processing 2/12: I dropped my phone. Is this covered?...\n",
            "Query: I dropped my phone. Is this covered?, Sentences Count: 746, Retrieved Context: ['NB: If your policy does not state that Mobile electronics are insured, then you are only insured for damage in your home to your mobile electronics as described in chapter 3.1 Basic coverage. Even if you have Allrisk insurance. Damage caused by For example, falling or bumping is not insured.', 'This is insured if your policy schedule states that Mobile electronics are insured. Contents insurance terms and conditions | 12/24 back to contents > 3.3 Glass optional coverage Does your policy state that Glass is insured? Then you are insured for glass breakage caused by any sudden and unforeseen events.', 'Then you are insured for damage to your contents as described in chapter 3.1 Basic coverage. Exclusions and limitations that also apply apply, are listed in the third column. Do you have Allrisk insurance?']...\n",
            "Context Relevance: 0.90\n",
            "Simple Retriever Score: 0.80\n",
            "Response Overall: 0.90\n",
            "\n",
            "Processing 3/12: My roof is leaking, causing damage to the ceiling....\n",
            "Query: My roof is leaking, causing damage to the ceiling. Is painting the ceiling insured?, Sentences Count: 746, Retrieved Context: ['Or do you have your belongings in a house with a thatched roof? Then we will not compensate your damage. 3 For what damage are you insured?', 'Will repair the damage yourself? Then you will receive compensation in money. Do you have optional Glass coverage insured?', 'We also reimburse the necessary painting work on the window of the broken window. By windows we mean: all glass windows in your home that are intended to provide light to pass through, including shower walls and shower doors, skylights, light panels of greenhouses and property boundaries. Glass breakage is not insured: - during the construction, renovation or extension of your home or apartment; - if your house or apartment is cracked; - from the moment your home or apartment empty for more than 3 months is inhabited or uninhabited.']...\n",
            "Context Relevance: 0.20\n",
            "Simple Retriever Score: 0.20\n",
            "Response Overall: 0.93\n",
            "\n",
            "Processing 4/12: I tripped outside and broke my glasses. Is this co...\n",
            "Query: I tripped outside and broke my glasses. Is this covered?, Sentences Count: 746, Retrieved Context: ['Then you are covered everywhere in and outside your home. worldwide insured against loss of or damage to the following valuables: - jewelry (glasses are not included)', 'Will repair the damage yourself? Then you will receive compensation in money. Do you have optional Glass coverage insured?', 'SPLITS_4 3.3 Glass Coverage Option Does your policy state that Glass is insured? Then you are insured for glass breakage caused by any sudden and unforeseen events. In the table below you can read what you are insured for.']...\n",
            "Context Relevance: 0.80\n",
            "Simple Retriever Score: 0.80\n",
            "Response Overall: 0.90\n",
            "\n",
            "Processing 5/12: My phone slipped out of my hands and fell into the...\n",
            "Query: My phone slipped out of my hands and fell into the water and was never found. Is this insured?, Sentences Count: 746, Retrieved Context: ['And has the water of this floods mixed? Then you are not insured. You are insured if there no mixing of water takes place and you only have damage caused by water from the flood by the collapse, overflow or failure of the non-primary water barrier.', 'This is insured if your policy schedule states that Mobile electronics are insured. Contents insurance terms and conditions | 12/24 back to contents > 3.3 Glass optional coverage Does your policy state that Glass is insured? Then you are insured for glass breakage caused by any sudden and unforeseen events.', 'NB: If your policy does not state that Mobile electronics are insured, then you are only insured for damage in your home to your mobile electronics as described in chapter 3.1 Basic coverage. Even if you have Allrisk insurance. Damage caused by For example, falling or bumping is not insured.']...\n",
            "Context Relevance: 0.50\n",
            "Simple Retriever Score: 0.50\n",
            "Response Overall: 0.82\n",
            "\n",
            "Processing 6/12: The microwave broke down due to a short circuit. I...\n",
            "Query: The microwave broke down due to a short circuit. Is this insured?, Sentences Count: 746, Retrieved Context: ['Damage caused by fire and explosion is insured;', 'This is insured if your policy schedule states that Mobile electronics are insured. Contents insurance terms and conditions | 12/24 back to contents > 3.3 Glass optional coverage Does your policy state that Glass is insured? Then you are insured for glass breakage caused by any sudden and unforeseen events.', 'What damages are not insured? The exclusions and limitations in Chapter 3.1 and the general exclusions mentioned in the General conditions also apply to damage to your mobile electronics. We also do not reimburse damage: - to borrowed, rented, lent or rented mobile electronics; - due to wear and tear or other slow-acting processes.']...\n",
            "Context Relevance: 0.80\n",
            "Simple Retriever Score: 0.30\n",
            "Response Overall: 0.90\n",
            "\n",
            "Processing 7/12: I sat on my glasses on the couch and broke them. I...\n",
            "Query: I sat on my glasses on the couch and broke them. Is this covered by insurance?, Sentences Count: 746, Retrieved Context: ['Will repair the damage yourself? Then you will receive compensation in money. Do you have optional Glass coverage insured?', 'Then you are covered everywhere in and outside your home. worldwide insured against loss of or damage to the following valuables: - jewelry (glasses are not included)', 'If your policy does not state that Glass is insured, then the glass in your home is not insured. But for events according to the basic coverage as mentioned in 3.1 Event Coverage Limitation Broken windows residential homeYou are insured for the replacement of your broken windows by glass of the same type and quality. We also reimburse the necessary painting work on the window of the broken window. By windows we mean: all glass windows in your home that are intended to provide light to pass through, including shower walls and shower doors, skylights, light panels of greenhouses and property boundaries. Glass breakage is not insured: - during the construction, renovation or extension of your home or apartment; - if your house or apartment is cracked; - from the moment your home or apartment empty for more than 3 months is inhabited or uninhabited. We do not reimburse damages caused by: - leakage of double glazing (condensation), without any glass breakage']...\n",
            "Context Relevance: 0.50\n",
            "Simple Retriever Score: 0.20\n",
            "Response Overall: 1.00\n",
            "\n",
            "Processing 8/12: My laptop was stolen from the trunk of my car. A w...\n",
            "Query: My laptop was stolen from the trunk of my car. A window was smashed. Is this insured?, Sentences Count: 746, Retrieved Context: ['Then you will receive compensation in money. Do you have optional Glass coverage insured? Then this also applies in case of broken windows.', 'You then pay the deductible to the repairer or to us. The voluntary deductible does not apply to broken windows (chapter 3.3). And not to the optional coverage Valuables loss and outdoors (chapter 3.5) and Mobile electronics (chapter 3.6).', 'Will repair the damage yourself? Then you will receive compensation in money. Do you have optional Glass coverage insured?']...\n",
            "Context Relevance: 0.20\n",
            "Simple Retriever Score: 0.70\n",
            "Response Overall: 0.95\n",
            "\n",
            "Processing 9/12: I lost an earring. I have no idea where it happene...\n",
            "Query: I lost an earring. I have no idea where it happened. Is this insured?, Sentences Count: 746, Retrieved Context: ['Borrowed or leased medical We do reimburse equipment (not hearing aids); - due to wear and other slow-acting processes. Or superficial damage that does not prevent use', 'worldwide insured against loss of or damage to the following valuables: - jewelry (glasses are not included) - art - collections - antique - photo and film equipment - musical instruments - diving, equestrian, parachute, golf and fishing equipment - medical equipment (hearing aids are not included) We will reimburse a maximum of the amount stated on your policy schedule for Loss of valuables and outside the home. policy schedule a voluntary deductible? Then this does not apply to this valuables coverage.', '- art - collections - antique - photo and film equipment - musical instruments - diving, equestrian, parachute, golf and fishing equipment - medical equipment (hearing aids are not included) We will reimburse a maximum of the amount stated on your policy schedule for Loss of valuables and outside the home. policy schedule a voluntary deductible? Then this does not apply to this valuables coverage. NB: You are only insured in your home for damage to your valuables as described in Chapter 3.1 Basic Coverage. If your policy schedule states that Valuables and loss are insured outside the home, then this cover also applies outside the home. Do you have Allrisk insurance? Then this allrisk coverage also applies to your valuables. What damages are not insured? The exclusions and limitations in Chapter 3.1 and the general exclusions mentioned in the General Terms and Conditions also apply to damage to your valuables. We also do not reimburse damage: - by the loss or misplacement of borrowed, rented or leased valuables. Borrowed or leased medical We do reimburse equipment (not hearing aids); - due to wear and other slow-acting processes. Or superficial damage that does not prevent use affect. Such as discoloration, aging, deformation, rot, corrosion, stains, scratches, scrapes, small dents;']...\n",
            "Context Relevance: 0.80\n",
            "Simple Retriever Score: 0.80\n",
            "Response Overall: 0.82\n",
            "\n",
            "Processing 10/12: A pan fell on the hob of my rental property. The l...\n",
            "Query: A pan fell on the hob of my rental property. The landlord is sending me to my own insurance. Is this covered?, Sentences Count: 746, Retrieved Context: ['Then you are insured for damage to your contents as described in chapter 3.1 Basic coverage. Exclusions and limitations that also apply apply, are listed in the third column. Do you have Allrisk insurance?', 'Then you are insured for damage to your contents as described in chapter 3.1 Basic coverage. Exclusions and limitations that also apply apply, are listed in the third column. Do you have Allrisk insurance?', 'Then this does not apply to this valuables coverage. NB: You are only insured in your home for damage to your valuables as described in Chapter 3.1 Basic Coverage. If your policy schedule states that Valuables and loss are insured outside the home, then this cover also applies outside the home.']...\n",
            "Context Relevance: 0.50\n",
            "Simple Retriever Score: 0.50\n",
            "Response Overall: 0.88\n",
            "\n",
            "Processing 11/12: Our dog chewed on the couch. Can I file for this d...\n",
            "Query: Our dog chewed on the couch. Can I file for this damage?, Sentences Count: 746, Retrieved Context: ['And also for damage caused by an accident that happens to your pet. Damage due to illness is not included here. A veterinarian must determine the injury.', \"house fall or collide and thereby you damage furnishings. We also reimburse the clean-up of the tree if it is yours and if it caused damage to the contents in your home. Remove roots and garden We do not reimburse ploughing. We also reimburse damage that occurs due to the fall or collapse of adjacent buildings or part thereof. Own risk: Damages your tenant's interest or owner's interest due to storm? Then you are deductible €75. 13. Pet accident You are insured for damage that you pet gets sick due to one of the causes which we mention in this table. And also for damage caused by an accident that happens to your pet. Damage due to illness is not included here. A veterinarian must determine the injury. This coverage only applies in your home and in your garden. We will reimburse a maximum of €2,500 for all pets together. Contents insurance conditions | 9/24 back to contents > Event Coverage Limitation or Exclusion 14. Explosion You are insured for damage caused by\", \"house fall or collide and thereby you damage furnishings. We also reimburse the clean-up of the tree if it is yours and if it caused damage to the contents in your home. Remove roots and garden We do not reimburse ploughing. We also reimburse damage that occurs due to the fall or collapse of adjacent buildings or part thereof. Own risk: Damages your tenant's interest or owner's interest due to storm? Then you are deductible €75. 13. Pet accident You are insured for damage that you pet gets sick due to one of the causes which we mention in this table. And also for damage caused by an accident that happens to your pet. Damage due to illness is not included here. A veterinarian must determine the injury. This coverage only applies in your home and in your garden. We will reimburse a maximum of €2,500 for all pets together. Contents insurance conditions | 9/24 back to contents > Event Coverage Limitation or Exclusion 14. Explosion You are insured for damage caused by\"]...\n",
            "Context Relevance: 0.30\n",
            "Simple Retriever Score: 0.30\n",
            "Response Overall: 1.00\n",
            "\n",
            "Processing 12/12: Due to a dishwasher leak, the laminate started to ...\n",
            "Query: Due to a dishwasher leak, the laminate started to bulge. Is this insured?, Sentences Count: 746, Retrieved Context: ['Will repair the damage yourself? Then you will receive compensation in money. Do you have optional Glass coverage insured?', 'This does not apply out: - whether or not you or another insured person was aware of this. - whether or not the damage was caused or arisen as a result.', '- Glass. This is only insured if it is stated on your policy schedule. You can read more about this in chapter 3 .']...\n",
            "Context Relevance: 0.40\n",
            "Simple Retriever Score: 0.20\n",
            "Response Overall: 0.83\n",
            "\n",
            "5. Calculating evaluation metrics...\n",
            "\n",
            "6. Generating evaluation report...\n",
            "\n",
            "# Merged Enhanced RAG Evaluation Report\n",
            "\n",
            "## Dataset Summary\n",
            "- Total samples evaluated: 12\n",
            "- Evaluation date: 2025-07-16 09:15:47\n",
            "- Average chunks retrieved per query: 5.0\n",
            "- Average sources used per query: 1.0\n",
            "\n",
            "## Context Retrieval Quality\n",
            "- Average Relevance: 0.567 (±0.264)\n",
            "- Average Coverage: 0.433\n",
            "- Average Noise Level: 0.517\n",
            "- Simple Retriever Score: 0.517\n",
            "\n",
            "## Response Generation Quality\n",
            "- Average Accuracy: 0.917\n",
            "- Average Completeness: 0.833\n",
            "- Average Relevance: 0.925\n",
            "- Average Conciseness: 0.967\n",
            "- **Overall Score: 0.911 (±0.067)**\n",
            "\n",
            "## Performance Distribution\n",
            "- Excellent (≥0.8): 100.0%\n",
            "- Good (0.6-0.8): 0.0%\n",
            "- Poor (<0.6): 0.0%\n",
            "\n",
            "## Top Performing Examples\n",
            "                                                                          question  response_overall_score                                                                                                                                                                                                                                                                                                                                                                                                                                                                     response_explanation\n",
            "0                             A ball went through my window. Will this be covered?                     1.0                                                                                                                                                                                                              The response accurately reflects the context provided, stating that if optional Glass coverage is insured, the damage would be covered. It fully addresses the question, is directly relevant, and concisely presents the necessary information without extraneous details.\n",
            "6   I sat on my glasses on the couch and broke them. Is this covered by insurance?                     1.0  The response accurately reflects the context provided, stating that personal items like glasses are not covered by the insurance policy described. It completely addresses the question by explaining the coverage limitations and specifically mentions that glasses are not included. The response is relevant as it uses the context appropriately to answer the question about insurance coverage. Additionally, the response is concise and clear without unnecessary information.\n",
            "10                        Our dog chewed on the couch. Can I file for this damage?                     1.0     The response accurately interprets the context provided, correctly identifying that the insurance covers accidents involving the pet, not accidents caused by the pet. It completely addresses the question by clarifying the distinction and directly answering whether the damage is covered. The response is relevant as it uses the context appropriately to determine coverage eligibility. It is concise, providing a clear and direct answer without unnecessary information.\n",
            "\n",
            "## Worst Performing Examples\n",
            "                                                                                          question  response_overall_score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          response_explanation\n",
            "4   My phone slipped out of my hands and fell into the water and was never found. Is this insured?                   0.825                       The response is mostly accurate in stating that the context does not explicitly cover the scenario of a phone falling into water and being lost. It correctly identifies the need to check the specific policy schedule for coverage details. However, it could have more explicitly stated that the context mentions mobile electronics are insured if specified in the policy schedule, which slightly affects completeness. The response is relevant as it directly addresses the question using the provided context. It is concise, avoiding unnecessary details while focusing on the key points.\n",
            "8                            I lost an earring. I have no idea where it happened. Is this insured?                   0.825                                               The response correctly identifies that the context does not explicitly cover personal items like an earring, which is accurate given the information provided. However, it could have been more complete by mentioning that jewelry is listed as insured under certain conditions, which might include earrings. The response is relevant and uses the context appropriately, focusing on the insurance coverage details. It is concise and avoids unnecessary information, providing a clear suggestion to check the policy schedule or contact the insurer for clarification.\n",
            "11                       Due to a dishwasher leak, the laminate started to bulge. Is this insured?                   0.830  The response accurately notes that the provided context does not specify coverage for damage caused by a dishwasher leak, which is correct based on the excerpts. It suggests checking other sections or contacting the insurer, which is a reasonable recommendation given the lack of specific information in the context. The response is relevant and concise, directly addressing the question without unnecessary details. However, it could be more complete by referencing general coverage for sudden and unforeseen events, which might include water damage, though this is not explicitly stated in the context.\n",
            "\n",
            "## Retrieval Analysis\n",
            "- Context relevance vs Response quality correlation: -0.148\n",
            "- Simple retriever vs Enhanced retriever correlation: 0.615\n",
            "- Sources most frequently used: {'indoor_asr.pdf': 12}\n",
            "\n",
            "\n",
            "Results saved to merged_evaluation_results/\n",
            "\n",
            "=== Evaluation Complete ===\n"
          ]
        }
      ]
    }
  ]
}