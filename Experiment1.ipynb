{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDtrXyUkXqpi",
        "outputId": "81bf0ca9-3e32-4ed4-c470-c9040f1e7a9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-dotenv in c:\\users\\chenz\\anaconda3\\lib\\site-packages (1.1.0)\n",
            "Requirement already satisfied: openai in c:\\users\\chenz\\anaconda3\\lib\\site-packages (2.8.1)\n",
            "Requirement already satisfied: faiss-cpu in c:\\users\\chenz\\anaconda3\\lib\\site-packages (1.13.0)\n",
            "Requirement already satisfied: sentence-transformers in c:\\users\\chenz\\anaconda3\\lib\\site-packages (5.1.2)\n",
            "Requirement already satisfied: pdfplumber in c:\\users\\chenz\\anaconda3\\lib\\site-packages (0.11.8)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from openai) (4.7.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from openai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: certifi in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from faiss-cpu) (2.1.3)\n",
            "Requirement already satisfied: packaging in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.57.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.9.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: pdfminer.six==20251107 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from pdfplumber) (20251107)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from pdfplumber) (5.1.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from pdfminer.six==20251107->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from pdfminer.six==20251107->pdfplumber) (44.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.21)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (72.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\chenz\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "%pip install python-dotenv openai faiss-cpu sentence-transformers pdfplumber\n",
        "from typing import List, Dict, Tuple\n",
        "from dotenv import load_dotenv\n",
        "from openai import AzureOpenAI\n",
        "import json\n",
        "import pdfplumber\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize Azure OpenAI client\n",
        "def init_client():\n",
        "    \"\"\"Initialize Azure OpenAI client with environment variables.\"\"\"\n",
        "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"AZURE_OPENAI_API_KEY environment variable not set.\")\n",
        "\n",
        "    endpoint = os.getenv(\"ENDPOINT_URL\")\n",
        "    if not endpoint:\n",
        "        raise ValueError(\"ENDPOINT_URL environment variable not set.\")\n",
        "\n",
        "    client = AzureOpenAI(\n",
        "        api_key=api_key,\n",
        "        api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
        "        azure_endpoint=endpoint\n",
        "    )\n",
        "    return client\n",
        "\n",
        "# Initialize client\n",
        "client = init_client()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sentence_chunks(sentences: List[str], chunk_size: int = 3, overlap: int = 1) -> List[str]:\n",
        "    \"\"\"Create overlapping sentence chunks\"\"\"\n",
        "    chunks = []\n",
        "    for i in range(0, len(sentences), chunk_size - overlap):\n",
        "        chunk_sentences = sentences[i:i + chunk_size]\n",
        "        if chunk_sentences:\n",
        "            chunks.append(' '.join(chunk_sentences))\n",
        "    return chunks\n",
        "\n",
        "def create_paragraph_chunks(paragraphs: List[str], max_length: int = 800, overlap: int = 100) -> List[str]:\n",
        "    \"\"\"Create overlapping paragraph chunks\"\"\"\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for para in paragraphs:\n",
        "        if len(current_chunk) + len(para) <= max_length:\n",
        "            current_chunk += para + \"\\n\\n\"\n",
        "        else:\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "                # Create overlap\n",
        "                current_chunk = current_chunk[-overlap:] + para + \"\\n\\n\"\n",
        "            else:\n",
        "                current_chunk = para + \"\\n\\n\"\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def create_semantic_chunks(text: str) -> List[str]:\n",
        "    \"\"\"Create chunks based on semantic sections (headers, numbered sections, etc.)\"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    # Pattern for detecting sections/headers\n",
        "    section_pattern = r'(?:^|\\n)(?:\\d+\\.?\\s+|[A-Z][^.]*:|\\n[A-Z][A-Z\\s]+\\n|Chapter\\s+\\d+|Section\\s+\\d+)'\n",
        "\n",
        "    sections = re.split(section_pattern, text)\n",
        "\n",
        "    for section in sections:\n",
        "        section = section.strip()\n",
        "        if len(section) > 100:  # Only keep substantial sections\n",
        "            chunks.append(section)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def create_advanced_chunks(text: str, page_num: int, source: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Create advanced chunks using multiple strategies:\n",
        "    1. Sentence-based chunking\n",
        "    2. Paragraph-based chunking\n",
        "    3. Overlapping chunks\n",
        "    4. Semantic chunks based on headers/sections\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    # Clean text\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Strategy 1: Sentence-based chunking\n",
        "    sentences = sent_tokenize(text)\n",
        "    sentence_chunks = create_sentence_chunks(sentences, chunk_size=3, overlap=1)\n",
        "\n",
        "    for i, chunk in enumerate(sentence_chunks):\n",
        "        chunks.append({\n",
        "            'text': chunk,\n",
        "            'source': source,\n",
        "            'page': page_num,\n",
        "            'chunk_id': f\"{source}_p{page_num}_sent_{i}\",\n",
        "            'chunk_type': 'sentence',\n",
        "            'length': len(chunk)\n",
        "        })\n",
        "\n",
        "    # Strategy 2: Paragraph-based chunking\n",
        "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "    paragraph_chunks = create_paragraph_chunks(paragraphs, max_length=800, overlap=100)\n",
        "\n",
        "    for i, chunk in enumerate(paragraph_chunks):\n",
        "        chunks.append({\n",
        "            'text': chunk,\n",
        "            'source': source,\n",
        "            'page': page_num,\n",
        "            'chunk_id': f\"{source}_p{page_num}_para_{i}\",\n",
        "            'chunk_type': 'paragraph',\n",
        "            'length': len(chunk)\n",
        "        })\n",
        "\n",
        "    # Strategy 3: Semantic chunking (based on headers/sections)\n",
        "    semantic_chunks = create_semantic_chunks(text)\n",
        "\n",
        "    for i, chunk in enumerate(semantic_chunks):\n",
        "        chunks.append({\n",
        "            'text': chunk,\n",
        "            'source': source,\n",
        "            'page': page_num,\n",
        "            'chunk_id': f\"{source}_p{page_num}_semantic_{i}\",\n",
        "            'chunk_type': 'semantic',\n",
        "            'length': len(chunk)\n",
        "        })\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def load_pdf_with_advanced_chunking(pdf_dir: str, pdf_files: List[str]) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Load PDFs with advanced chunking strategies for better retrieval.\n",
        "    Returns list of chunks with metadata.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        file_path = os.path.join(pdf_dir, pdf_file)\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Warning: {pdf_file} not found.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with pdfplumber.open(file_path) as pdf:\n",
        "                for page_num, page in enumerate(pdf.pages):\n",
        "                    text = page.extract_text() or \"\"\n",
        "                    if not text.strip():\n",
        "                        continue\n",
        "\n",
        "                    # Advanced chunking strategies\n",
        "                    page_chunks = create_advanced_chunks(text, page_num, pdf_file)\n",
        "                    chunks.extend(page_chunks)\n",
        "\n",
        "            print(f\"Extracted {len([c for c in chunks if c['source'] == pdf_file])} chunks from {pdf_file}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {pdf_file}: {str(e)}\")\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cZ_4dDmxwLfB"
      },
      "outputs": [],
      "source": [
        "# 将这段代码复制到您的notebook中\n",
        "import re\n",
        "\n",
        "# 增强的JSON提取函数\n",
        "def extract_json_from_response(response: str) -> str:\n",
        "    \"\"\"从响应中提取JSON，处理各种格式\"\"\"\n",
        "    # 处理markdown代码块\n",
        "    if '```json' in response:\n",
        "        match = re.search(r'```json\\s*\\n?(.*?)\\n?```', response, re.DOTALL)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "\n",
        "    if '```' in response:\n",
        "        match = re.search(r'```\\s*\\n?(.*?)\\n?```', response, re.DOTALL)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "\n",
        "    # 查找JSON对象\n",
        "    json_pattern = r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}'\n",
        "    matches = re.findall(json_pattern, response, re.DOTALL)\n",
        "    if matches:\n",
        "        return max(matches, key=len).strip()\n",
        "\n",
        "    return response.strip()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "d4143f55"
      },
      "outputs": [],
      "source": [
        "def load_evaluation_dataset(file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load evaluation dataset containing questions, ground truth answers, and retrieved contexts.\n",
        "\n",
        "    Expected format:\n",
        "    - CSV/Excel with columns: 'question', 'ground_truth', 'retrieved_context'\n",
        "    - Or JSON with same structure\n",
        "    \"\"\"\n",
        "    if file_path.endswith('.csv'):\n",
        "        df = pd.read_csv(file_path)\n",
        "    elif file_path.endswith('.xlsx'):\n",
        "        df = pd.read_excel(file_path)\n",
        "    elif file_path.endswith('.json'):\n",
        "        df = pd.read_json(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Use CSV, Excel, or JSON.\")\n",
        "\n",
        "    # Validate required columns\n",
        "    required_columns = ['question', 'ground_truth']\n",
        "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "    if missing_columns:\n",
        "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
        "\n",
        "    print(f\"Loaded {len(df)} evaluation samples\")\n",
        "    print(f\"Sample data:\\n{df.head(2)}\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KL7dpa0xaq_W"
      },
      "outputs": [],
      "source": [
        "def generate_response_with_context(question: str, context: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate a response using the provided context.\n",
        "    \"\"\"\n",
        "    deployment = os.getenv(\"DEPLOYMENT_NAME\")\n",
        "    if not deployment:\n",
        "        raise ValueError(\"DEPLOYMENT_NAME is missing in .env\")\n",
        "\n",
        "    prompt = f\"\"\"Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Please answer the question based on the provided context. Be concise and accurate.\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=deployment,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant that answers questions based on the provided context.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=150,\n",
        "        temperature=0.3  # Lower temperature for more consistent responses\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_2I06utWawDw"
      },
      "outputs": [],
      "source": [
        "def judge_response_quality(question: str, response: str, ground_truth: str, context: str) -> Dict[str, any]:\n",
        "    \"\"\"\n",
        "    Evaluate response quality against ground truth using multiple criteria.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"You are an expert evaluator. Please evaluate the following response across multiple dimensions:\n",
        "\n",
        "Question: {question}\n",
        "Context Provided: {context}\n",
        "Generated Response: {response}\n",
        "Ground Truth Answer: {ground_truth}\n",
        "\n",
        "Please evaluate on these criteria (each on a scale of 0-1):\n",
        "1. **Accuracy**: How factually correct is the response compared to the ground truth?\n",
        "2. **Completeness**: Does the response cover all important points from the ground truth?\n",
        "3. **Relevance**: Is the response relevant to the question and uses the context appropriately?\n",
        "4. **Conciseness**: Is the response clear and concise without unnecessary information?\n",
        "\n",
        "Provide your evaluation in the following JSON format:\n",
        "{{\n",
        "    \"accuracy\": <float>,\n",
        "    \"completeness\": <float>,\n",
        "    \"relevance\": <float>,\n",
        "    \"conciseness\": <float>,\n",
        "    \"overall_score\": <float>,\n",
        "    \"explanation\": \"<brief explanation of the scores>\"\n",
        "}}\"\"\"\n",
        "\n",
        "    deployment = os.getenv(\"DEPLOYMENT_NAME\")\n",
        "    completion = client.chat.completions.create(\n",
        "        model=deployment,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=300,\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    result = completion.choices[0].message.content.strip()\n",
        "\n",
        "    try:\n",
        "        # Parse JSON response\n",
        "        # 提取JSON内容（去除markdown标记）\n",
        "      if '```json' in result:\n",
        "        json_content = result.split('```json')[1].split('```')[0].strip()\n",
        "      elif '```' in result:\n",
        "        json_content = result.split('```')[1].split('```')[0].strip()\n",
        "      else:\n",
        "        json_content = result\n",
        "\n",
        "# 解析JSON\n",
        "      evaluation = json.loads(json_content)\n",
        "      return evaluation\n",
        "    except json.JSONDecodeError:\n",
        "        # Fallback parsing if JSON fails\n",
        "        print(f\"Failed to parse JSON response: {result}\")\n",
        "        return {\n",
        "            \"accuracy\": 0.0,\n",
        "            \"completeness\": 0.0,\n",
        "            \"relevance\": 0.0,\n",
        "            \"conciseness\": 0.0,\n",
        "            \"overall_score\": 0.0,\n",
        "            \"explanation\": \"Failed to parse evaluation response\"\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Q8Q8KNeFa23c"
      },
      "outputs": [],
      "source": [
        "def judge_context_relevance(question: str, context: str) -> Dict[str, any]:\n",
        "    \"\"\"\n",
        "    Evaluate how relevant the retrieved context is to the question.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"You are an expert evaluator. Please evaluate how relevant the following context is to the question.\n",
        "\n",
        "Question: {question}\n",
        "Retrieved Context: {context}\n",
        "\n",
        "Please evaluate:\n",
        "1. **Relevance Score** (0-1): How relevant is the context to answering the question?\n",
        "2. **Coverage Score** (0-1): How well does the context cover the information needed to answer the question?\n",
        "3. **Noise Level** (0-1): How much irrelevant information is in the context? (0 = lots of noise, 1 = no noise)\n",
        "\n",
        "Provide your evaluation in the following JSON format:\n",
        "{{\n",
        "    \"relevance_score\": <float>,\n",
        "    \"coverage_score\": <float>,\n",
        "    \"noise_level\": <float>,\n",
        "    \"explanation\": \"<brief explanation>\"\n",
        "}}\"\"\"\n",
        "\n",
        "    deployment = os.getenv(\"DEPLOYMENT_NAME\")\n",
        "    completion = client.chat.completions.create(\n",
        "        model=deployment,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=200,\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    result = completion.choices[0].message.content.strip()\n",
        "\n",
        "    try:\n",
        "        json_content = extract_json_from_response(result)\n",
        "        evaluation = json.loads(json_content)\n",
        "        # Ensure all required fields exist\n",
        "        required_fields = ['relevance_score', 'coverage_score', 'noise_level', 'explanation']\n",
        "        for field in required_fields:\n",
        "            if field not in evaluation:\n",
        "                evaluation[field] = 0.0 if field != 'explanation' else 'Field missing from evaluation'\n",
        "        return evaluation\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Failed to parse JSON response: {result}\")\n",
        "        return {\n",
        "            \"relevance_score\": 0.0,\n",
        "            \"coverage_score\": 0.0,\n",
        "            \"noise_level\": 0.0,\n",
        "            \"explanation\": \"Failed to parse evaluation response\"\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LTHkliJIa9gJ"
      },
      "outputs": [],
      "source": [
        "def run_baseline_evaluation(df: pd.DataFrame, sample_size: int = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Run comprehensive baseline evaluation on the dataset.\n",
        "    \"\"\"\n",
        "    if sample_size:\n",
        "        df = df.sample(n=min(sample_size, len(df)), random_state=42)\n",
        "        print(f\"Evaluating {len(df)} samples...\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        print(f\"\\nProcessing {idx + 1}/{len(df)}...\")\n",
        "\n",
        "        question = row['question']\n",
        "        ground_truth = row['ground_truth']\n",
        "        context = row['retrieved_context']\n",
        "\n",
        "        # 1. Evaluate context relevance\n",
        "        context_eval = judge_context_relevance(question, context)\n",
        "\n",
        "        # 2. Generate response using context\n",
        "        generated_response = generate_response_with_context(question, context)\n",
        "\n",
        "        # 3. Evaluate response quality\n",
        "        response_eval = judge_response_quality(question, generated_response, ground_truth, context)\n",
        "\n",
        "        # Compile results\n",
        "        result = {\n",
        "            'question': question,\n",
        "            'ground_truth': ground_truth,\n",
        "            'retrieved_context': context[:200] + '...' if len(context) > 200 else context,\n",
        "            'generated_response': generated_response,\n",
        "\n",
        "            # Context evaluation metrics\n",
        "            'context_relevance': context_eval.get('relevance_score', 0.0),\n",
        "            'context_coverage': context_eval.get('coverage_score', 0.0),\n",
        "            'context_noise_level': context_eval.get('noise_level', 0.0),\n",
        "            'context_explanation': context_eval.get('explanation', 'N/A'),\n",
        "\n",
        "            # Response evaluation metrics\n",
        "            'response_accuracy': response_eval.get('accuracy', 0.0),\n",
        "            'response_completeness': response_eval.get('completeness', 0.0),\n",
        "            'response_relevance': response_eval.get('relevance', 0.0),  # 注意这里是 'relevance' 不是 'response_relevance'\n",
        "            'response_conciseness': response_eval.get('conciseness', 0.0),\n",
        "            'response_overall_score': response_eval.get('overall_score', 0.0),\n",
        "            'response_explanation': response_eval.get('explanation', 'N/A')\n",
        "        }\n",
        "\n",
        "        results.append(result)\n",
        "\n",
        "        # Print summary for this sample\n",
        "        print(f\"Context Relevance: {context_eval['relevance_score']:.2f}\")\n",
        "        print(f\"Response Overall Score: {response_eval['overall_score']:.2f}\")\n",
        "\n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-mNGk5z6bTwS"
      },
      "outputs": [],
      "source": [
        "def calculate_baseline_metrics(results_df: pd.DataFrame) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculate aggregate baseline metrics from evaluation results.\n",
        "    \"\"\"\n",
        "    metrics = {\n",
        "        # Context metrics\n",
        "        'avg_context_relevance': results_df['context_relevance'].mean(),\n",
        "        'avg_context_coverage': results_df['context_coverage'].mean(),\n",
        "        'avg_context_noise_level': results_df['context_noise_level'].mean(),\n",
        "\n",
        "        # Response metrics\n",
        "        'avg_response_accuracy': results_df['response_accuracy'].mean(),\n",
        "        'avg_response_completeness': results_df['response_completeness'].mean(),\n",
        "        'avg_response_relevance': results_df['response_relevance'].mean(),\n",
        "        'avg_response_conciseness': results_df['response_conciseness'].mean(),\n",
        "        'avg_response_overall': results_df['response_overall_score'].mean(),\n",
        "\n",
        "        # Standard deviations\n",
        "        'std_context_relevance': results_df['context_relevance'].std(),\n",
        "        'std_response_overall': results_df['response_overall_score'].std(),\n",
        "\n",
        "        # Performance distribution\n",
        "        'excellent_responses': (results_df['response_overall_score'] >= 0.8).sum() / len(results_df),\n",
        "        'good_responses': ((results_df['response_overall_score'] >= 0.6) &\n",
        "                          (results_df['response_overall_score'] < 0.8)).sum() / len(results_df),\n",
        "        'poor_responses': (results_df['response_overall_score'] < 0.6).sum() / len(results_df)\n",
        "    }\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OCm_nZfSbkgE"
      },
      "outputs": [],
      "source": [
        "def generate_evaluation_report(results_df: pd.DataFrame, metrics: Dict[str, float], output_dir: str = '.'):\n",
        "    \"\"\"\n",
        "    Generate comprehensive evaluation report.\n",
        "    \"\"\"\n",
        "    # Save detailed results\n",
        "    results_df.to_csv(f\"{output_dir}/baseline_evaluation_detailed.csv\", index=False)\n",
        "\n",
        "    # Create summary report\n",
        "    report = f\"\"\"\n",
        "# LLM-as-a-Judge Baseline Evaluation Report\n",
        "\n",
        "## Dataset Summary\n",
        "- Total samples evaluated: {len(results_df)}\n",
        "- Evaluation date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "## Context Retrieval Quality\n",
        "- Average Relevance: {metrics['avg_context_relevance']:.3f} (±{metrics['std_context_relevance']:.3f})\n",
        "- Average Coverage: {metrics['avg_context_coverage']:.3f}\n",
        "- Average Noise Level: {metrics['avg_context_noise_level']:.3f}\n",
        "\n",
        "## Response Generation Quality\n",
        "- Average Accuracy: {metrics['avg_response_accuracy']:.3f}\n",
        "- Average Completeness: {metrics['avg_response_completeness']:.3f}\n",
        "- Average Relevance: {metrics['avg_response_relevance']:.3f}\n",
        "- Average Conciseness: {metrics['avg_response_conciseness']:.3f}\n",
        "- **Overall Score: {metrics['avg_response_overall']:.3f} (±{metrics['std_response_overall']:.3f})**\n",
        "\n",
        "## Performance Distribution\n",
        "- Excellent (≥0.8): {metrics['excellent_responses']:.1%}\n",
        "- Good (0.6-0.8): {metrics['good_responses']:.1%}\n",
        "- Poor (<0.6): {metrics['poor_responses']:.1%}\n",
        "\n",
        "## Top Performing Examples\n",
        "{results_df.nlargest(3, 'response_overall_score')[['question', 'response_overall_score', 'response_explanation']].to_string()}\n",
        "\n",
        "## Worst Performing Examples\n",
        "{results_df.nsmallest(3, 'response_overall_score')[['question', 'response_overall_score', 'response_explanation']].to_string()}\n",
        "\"\"\"\n",
        "\n",
        "    # Save report\n",
        "    with open(f\"{output_dir}/baseline_evaluation_report.txt\", 'w') as f:\n",
        "        f.write(report)\n",
        "\n",
        "    # Save metrics as JSON\n",
        "    with open(f\"{output_dir}/baseline_metrics.json\", 'w') as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "\n",
        "    print(report)\n",
        "    print(f\"\\nResults saved to {output_dir}/\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple retrieve context function from openai_llm_as_a_judge\n",
        "def retrieve_simple_context(query: str, embedder: SentenceTransformer, chunks: List[Dict], k: int = 3) -> List[str]:\n",
        "    \"\"\"Simple context retrieval function from openai_llm_as_a_judge\"\"\"\n",
        "    sentences = [chunk['text'] for chunk in chunks]\n",
        "    if not sentences:\n",
        "        print(\"Warning: No valid sentences extracted from chunks.\")\n",
        "        return [\"No relevant context found.\"]\n",
        "\n",
        "    embeddings = embedder.encode(sentences, convert_to_numpy=True)\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    index.add(embeddings)\n",
        "\n",
        "    query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
        "    distances, indices = index.search(query_embedding, k=k)\n",
        "\n",
        "    retrieved_context = [sentences[i] for i in indices[0] if i < len(sentences)]\n",
        "    print(f\"Query: {query}, Sentences Count: {len(sentences)}, Retrieved Context: {retrieved_context[:100]}...\")\n",
        "\n",
        "    return retrieved_context if retrieved_context else [\"No relevant context found.\"]\n",
        "    \n",
        "def retrieve_enhanced_context(query: str, index: faiss.IndexFlatL2, embedder: SentenceTransformer,\n",
        "                            chunks: List[Dict], k: int = 8) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Enhanced context retrieval with multiple strategies:\n",
        "    1. Semantic similarity search\n",
        "    2. Keyword matching\n",
        "    3. Diversity-based reranking\n",
        "    \"\"\"\n",
        "\n",
        "    # Semantic search\n",
        "    query_embedding = embedder.encode([query], convert_to_tensor=False)\n",
        "    distances, indices = index.search(np.array(query_embedding, dtype=np.float32), k * 2)\n",
        "\n",
        "    # Get candidate chunks\n",
        "    candidate_chunks = []\n",
        "    for i in indices[0]:\n",
        "        if i < len(chunks):\n",
        "            chunk = chunks[i].copy()\n",
        "            chunk['similarity_score'] = float(1 / (1 + distances[0][len(candidate_chunks)]))\n",
        "            candidate_chunks.append(chunk)\n",
        "\n",
        "    # Keyword matching boost\n",
        "    query_keywords = set(query.lower().split())\n",
        "    for chunk in candidate_chunks:\n",
        "        chunk_keywords = set(chunk['text'].lower().split())\n",
        "        keyword_overlap = len(query_keywords.intersection(chunk_keywords))\n",
        "        chunk['keyword_score'] = keyword_overlap / len(query_keywords) if query_keywords else 0\n",
        "\n",
        "    # Diversity-based reranking\n",
        "    final_chunks = rerank_for_diversity(candidate_chunks, k)\n",
        "\n",
        "    return final_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_enhanced_vector_index(chunks: List[Dict]) -> Tuple[faiss.IndexFlatL2, SentenceTransformer, List[Dict]]:\n",
        "    \"\"\"Build enhanced vector index with multiple embedding strategies\"\"\"\n",
        "\n",
        "    # Initialize embedder\n",
        "    embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    # Extract texts for embedding\n",
        "    texts = [chunk['text'] for chunk in chunks]\n",
        "\n",
        "    # Create embeddings\n",
        "    embeddings = embedder.encode(texts, convert_to_tensor=False, show_progress_bar=True)\n",
        "\n",
        "    # Create FAISS index\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(np.array(embeddings, dtype=np.float32))\n",
        "\n",
        "    print(f\"Built vector index with {len(chunks)} chunks\")\n",
        "\n",
        "    return index, embedder, chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rerank_for_diversity(chunks: List[Dict], k: int) -> List[Dict]:\n",
        "    \"\"\"Rerank chunks to ensure diversity in sources and content\"\"\"\n",
        "\n",
        "    # Sort by combined score (similarity + keyword)\n",
        "    for chunk in chunks:\n",
        "        chunk['combined_score'] = chunk['similarity_score'] + chunk['keyword_score'] * 0.3\n",
        "\n",
        "    chunks.sort(key=lambda x: x['combined_score'], reverse=True)\n",
        "\n",
        "    # Ensure diversity\n",
        "    selected_chunks = []\n",
        "    used_sources = set()\n",
        "    used_chunk_types = set()\n",
        "\n",
        "    for chunk in chunks:\n",
        "        if len(selected_chunks) >= k:\n",
        "            break\n",
        "\n",
        "        # Prioritize diversity in sources and chunk types\n",
        "        source_penalty = 0.1 if chunk['source'] in used_sources else 0\n",
        "        type_penalty = 0.05 if chunk['chunk_type'] in used_chunk_types else 0\n",
        "\n",
        "        chunk['final_score'] = chunk['combined_score'] - source_penalty - type_penalty\n",
        "\n",
        "        selected_chunks.append(chunk)\n",
        "        used_sources.add(chunk['source'])\n",
        "        used_chunk_types.add(chunk['chunk_type'])\n",
        "\n",
        "    return selected_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_enhanced_context_for_df(df: pd.DataFrame, index, embedder, chunks, k = 5):\n",
        "    final_chunks = df.apply(lambda row: retrieve_enhanced_context(row[\"question\"], index, embedder, chunks, k), axis=1)\n",
        "    df[\"retrieved_context\"] = final_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zrND7m3Abtzq",
        "outputId": "2b0e44a0-6635-4a76-d5c0-dd97d23b02f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 33 evaluation samples\n",
            "Sample data:\n",
            "                                            question  \\\n",
            "0  I have a concussion, I want to cancel, is this...   \n",
            "1  There is a strike on the German railways next ...   \n",
            "\n",
            "                                        ground_truth  \n",
            "0  Yes, if you have to cancel your trip due to a ...  \n",
            "1  No, you cannot cancel your trip due to a strik...  \n",
            "Extracted 510 chunks from aegon_travel.pdf\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c670988474a43f6a2e3967ea7855eef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Built vector index with 510 chunks\n",
            "\n",
            "Starting baseline evaluation...\n",
            "\n",
            "Processing 1/33...\n",
            "Context Relevance: 0.30\n",
            "Response Overall Score: 0.47\n",
            "\n",
            "Processing 2/33...\n",
            "Context Relevance: 0.10\n",
            "Response Overall Score: 0.45\n",
            "\n",
            "Processing 3/33...\n",
            "Context Relevance: 0.80\n",
            "Response Overall Score: 0.80\n",
            "\n",
            "Processing 4/33...\n",
            "Context Relevance: 0.70\n",
            "Response Overall Score: 0.62\n",
            "\n",
            "Processing 5/33...\n",
            "Context Relevance: 0.10\n",
            "Response Overall Score: 0.62\n",
            "\n",
            "Processing 6/33...\n",
            "Context Relevance: 0.80\n",
            "Response Overall Score: 0.33\n",
            "\n",
            "Processing 7/33...\n",
            "Context Relevance: 0.10\n",
            "Response Overall Score: 0.62\n",
            "\n",
            "Processing 8/33...\n",
            "Context Relevance: 0.10\n",
            "Response Overall Score: 0.82\n",
            "\n",
            "Processing 9/33...\n",
            "Context Relevance: 0.10\n",
            "Response Overall Score: 0.95\n",
            "\n",
            "Processing 10/33...\n",
            "Context Relevance: 0.10\n",
            "Response Overall Score: 0.97\n",
            "\n",
            "Processing 11/33...\n",
            "Context Relevance: 0.20\n",
            "Response Overall Score: 0.62\n",
            "\n",
            "Processing 12/33...\n",
            "Context Relevance: 0.20\n",
            "Response Overall Score: 0.62\n",
            "\n",
            "Processing 13/33...\n",
            "Context Relevance: 0.80\n",
            "Response Overall Score: 0.82\n",
            "\n",
            "Processing 14/33...\n",
            "Context Relevance: 0.20\n",
            "Response Overall Score: 0.62\n",
            "\n",
            "Processing 15/33...\n",
            "Context Relevance: 0.70\n",
            "Response Overall Score: 1.00\n",
            "\n",
            "Processing 16/33...\n",
            "Context Relevance: 0.60\n",
            "Response Overall Score: 0.78\n",
            "\n",
            "Processing 17/33...\n",
            "Context Relevance: 0.80\n",
            "Response Overall Score: 0.97\n",
            "\n",
            "Processing 18/33...\n",
            "Context Relevance: 0.80\n",
            "Response Overall Score: 0.90\n",
            "\n",
            "Processing 19/33...\n",
            "Context Relevance: 0.10\n",
            "Response Overall Score: 0.57\n",
            "\n",
            "Processing 20/33...\n",
            "Context Relevance: 0.20\n",
            "Response Overall Score: 0.65\n",
            "\n",
            "Processing 21/33...\n",
            "Context Relevance: 0.20\n",
            "Response Overall Score: 0.47\n",
            "\n",
            "Processing 22/33...\n",
            "Context Relevance: 0.30\n",
            "Response Overall Score: 0.42\n",
            "\n",
            "Processing 23/33...\n",
            "Context Relevance: 0.20\n",
            "Response Overall Score: 0.62\n",
            "\n",
            "Processing 24/33...\n",
            "Context Relevance: 0.30\n",
            "Response Overall Score: 0.82\n",
            "\n",
            "Processing 25/33...\n",
            "Context Relevance: 0.10\n",
            "Response Overall Score: 0.60\n",
            "\n",
            "Processing 26/33...\n",
            "Context Relevance: 0.20\n",
            "Response Overall Score: 0.57\n",
            "\n",
            "Processing 27/33...\n",
            "Context Relevance: 0.20\n",
            "Response Overall Score: 0.80\n",
            "\n",
            "Processing 28/33...\n",
            "Context Relevance: 0.80\n",
            "Response Overall Score: 0.68\n",
            "\n",
            "Processing 29/33...\n",
            "Context Relevance: 0.20\n",
            "Response Overall Score: 0.57\n",
            "\n",
            "Processing 30/33...\n",
            "Context Relevance: 0.60\n",
            "Response Overall Score: 0.65\n",
            "\n",
            "Processing 31/33...\n",
            "Context Relevance: 0.70\n",
            "Response Overall Score: 0.78\n",
            "\n",
            "Processing 32/33...\n",
            "Context Relevance: 0.20\n",
            "Response Overall Score: 0.68\n",
            "\n",
            "Processing 33/33...\n",
            "Context Relevance: 0.70\n",
            "Response Overall Score: 0.57\n",
            "\n",
            "Calculating baseline metrics...\n",
            "\n",
            "Generating evaluation report...\n",
            "\n",
            "# LLM-as-a-Judge Baseline Evaluation Report\n",
            "\n",
            "## Dataset Summary\n",
            "- Total samples evaluated: 33\n",
            "- Evaluation date: 2025-11-24 22:26:13\n",
            "\n",
            "## Context Retrieval Quality\n",
            "- Average Relevance: 0.379 (±0.281)\n",
            "- Average Coverage: 0.297\n",
            "- Average Noise Level: 0.379\n",
            "\n",
            "## Response Generation Quality\n",
            "- Average Accuracy: 0.588\n",
            "- Average Completeness: 0.518\n",
            "- Average Relevance: 0.773\n",
            "- Average Conciseness: 0.848\n",
            "- **Overall Score: 0.682 (±0.169)**\n",
            "\n",
            "## Performance Distribution\n",
            "- Excellent (≥0.8): 30.3%\n",
            "- Good (0.6-0.8): 42.4%\n",
            "- Poor (<0.6): 27.3%\n",
            "\n",
            "## Top Performing Examples\n",
            "                                                                    question  response_overall_score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 response_explanation\n",
            "14  I rented skis and they are damaged. I have to pay €400, is this insured?                   1.000                                                                                                                                                                          The generated response accurately states that damage to rented skis is not insured, aligning perfectly with the ground truth answer. It covers all necessary points, is directly relevant to the question, and uses the context appropriately. The response is clear and concise, providing only the essential information without any superfluous details.\n",
            "9                     I bought paracetamol at the pharmacy, is this covered?                   0.975  The generated response accurately states that purchasing paracetamol at a pharmacy is not covered by the insurance policy, aligning with the ground truth. It is complete as it addresses the key point of the ground truth answer. The relevance is slightly lower because the response could more directly state the lack of coverage without referencing the context, which does not explicitly mention medication coverage. The response is clear and concise, directly answering the question without unnecessary information.\n",
            "16           My suitcase fell, it has scratches and a dent. Is this insured?                   0.975                                                                                     The generated response accurately states that scratches and dents to suitcases are not insured, which aligns with the ground truth. It is relevant to the question and uses the context appropriately. The response is concise and clear, providing the necessary information without any extraneous details. However, it slightly lacks completeness as it does not explicitly mention 'caused by falls,' which is present in the ground truth.\n",
            "\n",
            "## Worst Performing Examples\n",
            "                                                                                                                  question  response_overall_score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              response_explanation\n",
            "5       was standing in a bar I turned around and before I knew it my bag had been stolen from the stool, is this covered?                   0.325                                                                                                                                                                The response inaccurately states that the theft is covered, contradicting the ground truth that specifies theft without direct supervision is not covered. Completeness is low as it fails to mention the supervision condition. Relevance is moderate as it addresses the question but misinterprets the context. Conciseness is high as the response is clear and lacks unnecessary information.\n",
            "21  I had to go back because my neighbor passed away and I want to go to the funeral, can I go home early and submit this?                   0.425                                                                                              The generated response does not accurately reflect the ground truth answer, which states that reasonable additional costs for returning due to a neighbor's death are covered. The response lacks completeness as it misses this key point. However, it is somewhat relevant as it discusses checking the policy details, which is a reasonable suggestion given the context. The response is concise, providing a clear suggestion without unnecessary information.\n",
            "1                                                        There is a strike on the German railways next week, can I cancel?                   0.450  The generated response correctly identifies that the provided context does not contain specific information about cancellations due to a strike on the German railways. However, it does not provide the factual answer given in the ground truth, which states that cancellations due to strikes are not insured. The response is relevant in suggesting checking the travel insurance policy, but it lacks completeness in addressing the specific policy exclusion mentioned in the ground truth. The response is concise and avoids unnecessary information.\n",
            "\n",
            "\n",
            "Results saved to evaluation_results/\n"
          ]
        }
      ],
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your evaluation dataset\n",
        "    # Replace with your actual data file path\n",
        "    dataset_path = \"aegon_travel.xlsx\"  # or .xlsx, .json\n",
        "\n",
        "    try:\n",
        "        # Load data\n",
        "        df = load_evaluation_dataset(dataset_path)\n",
        "\n",
        "        pdf_dir = \".\"\n",
        "        pdf_files = [\"aegon_travel.pdf\"]\n",
        "        chunks = load_pdf_with_advanced_chunking(pdf_dir, pdf_files)\n",
        "\n",
        "        index, embedder, chunks = build_enhanced_vector_index(chunks)\n",
        "\n",
        "        retrieve_enhanced_context_for_df(df, index, embedder, chunks, k = 5)\n",
        "\n",
        "        # Run evaluation (you can specify sample_size for testing)\n",
        "        print(\"\\nStarting baseline evaluation...\")\n",
        "        results_df = run_baseline_evaluation(df, sample_size=None)  # Use sample_size=10 for testing\n",
        "\n",
        "        # Calculate metrics\n",
        "        print(\"\\nCalculating baseline metrics...\")\n",
        "        metrics = calculate_baseline_metrics(results_df)\n",
        "\n",
        "        # Generate report\n",
        "        print(\"\\nGenerating evaluation report...\")\n",
        "        import os\n",
        "        output_dir = 'evaluation_results'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        generate_evaluation_report(results_df, metrics, output_dir=output_dir)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during evaluation: {str(e)}\")\n",
        "        raise"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
