{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDtrXyUkXqpi",
        "outputId": "81bf0ca9-3e32-4ed4-c470-c9040f1e7a9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.94.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "%pip install python-dotenv openai faiss-cpu sentence-transformers pdfplumber\n",
        "from typing import List, Dict, Tuple\n",
        "from dotenv import load_dotenv\n",
        "from openai import AzureOpenAI\n",
        "import json\n",
        "import pdfplumber\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize Azure OpenAI client\n",
        "def init_client():\n",
        "    \"\"\"Initialize Azure OpenAI client with environment variables.\"\"\"\n",
        "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"AZURE_OPENAI_API_KEY environment variable not set.\")\n",
        "\n",
        "    endpoint = os.getenv(\"ENDPOINT_URL\")\n",
        "    if not endpoint:\n",
        "        raise ValueError(\"ENDPOINT_URL environment variable not set.\")\n",
        "\n",
        "    client = AzureOpenAI(\n",
        "        api_key=api_key,\n",
        "        api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
        "        azure_endpoint=endpoint\n",
        "    )\n",
        "    return client\n",
        "\n",
        "# Initialize client\n",
        "client = init_client()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sentence_chunks(sentences: List[str], chunk_size: int = 3, overlap: int = 1) -> List[str]:\n",
        "    \"\"\"Create overlapping sentence chunks\"\"\"\n",
        "    chunks = []\n",
        "    for i in range(0, len(sentences), chunk_size - overlap):\n",
        "        chunk_sentences = sentences[i:i + chunk_size]\n",
        "        if chunk_sentences:\n",
        "            chunks.append(' '.join(chunk_sentences))\n",
        "    return chunks\n",
        "\n",
        "def create_paragraph_chunks(paragraphs: List[str], max_length: int = 800, overlap: int = 100) -> List[str]:\n",
        "    \"\"\"Create overlapping paragraph chunks\"\"\"\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for para in paragraphs:\n",
        "        if len(current_chunk) + len(para) <= max_length:\n",
        "            current_chunk += para + \"\\n\\n\"\n",
        "        else:\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "                # Create overlap\n",
        "                current_chunk = current_chunk[-overlap:] + para + \"\\n\\n\"\n",
        "            else:\n",
        "                current_chunk = para + \"\\n\\n\"\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def create_semantic_chunks(text: str) -> List[str]:\n",
        "    \"\"\"Create chunks based on semantic sections (headers, numbered sections, etc.)\"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    # Pattern for detecting sections/headers\n",
        "    section_pattern = r'(?:^|\\n)(?:\\d+\\.?\\s+|[A-Z][^.]*:|\\n[A-Z][A-Z\\s]+\\n|Chapter\\s+\\d+|Section\\s+\\d+)'\n",
        "\n",
        "    sections = re.split(section_pattern, text)\n",
        "\n",
        "    for section in sections:\n",
        "        section = section.strip()\n",
        "        if len(section) > 100:  # Only keep substantial sections\n",
        "            chunks.append(section)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def create_advanced_chunks(text: str, page_num: int, source: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Create advanced chunks using multiple strategies:\n",
        "    1. Sentence-based chunking\n",
        "    2. Paragraph-based chunking\n",
        "    3. Overlapping chunks\n",
        "    4. Semantic chunks based on headers/sections\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    # Clean text\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Strategy 1: Sentence-based chunking\n",
        "    sentences = sent_tokenize(text)\n",
        "    sentence_chunks = create_sentence_chunks(sentences, chunk_size=3, overlap=1)\n",
        "\n",
        "    for i, chunk in enumerate(sentence_chunks):\n",
        "        chunks.append({\n",
        "            'text': chunk,\n",
        "            'source': source,\n",
        "            'page': page_num,\n",
        "            'chunk_id': f\"{source}_p{page_num}_sent_{i}\",\n",
        "            'chunk_type': 'sentence',\n",
        "            'length': len(chunk)\n",
        "        })\n",
        "\n",
        "    # Strategy 2: Paragraph-based chunking\n",
        "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "    paragraph_chunks = create_paragraph_chunks(paragraphs, max_length=800, overlap=100)\n",
        "\n",
        "    for i, chunk in enumerate(paragraph_chunks):\n",
        "        chunks.append({\n",
        "            'text': chunk,\n",
        "            'source': source,\n",
        "            'page': page_num,\n",
        "            'chunk_id': f\"{source}_p{page_num}_para_{i}\",\n",
        "            'chunk_type': 'paragraph',\n",
        "            'length': len(chunk)\n",
        "        })\n",
        "\n",
        "    # Strategy 3: Semantic chunking (based on headers/sections)\n",
        "    semantic_chunks = create_semantic_chunks(text)\n",
        "\n",
        "    for i, chunk in enumerate(semantic_chunks):\n",
        "        chunks.append({\n",
        "            'text': chunk,\n",
        "            'source': source,\n",
        "            'page': page_num,\n",
        "            'chunk_id': f\"{source}_p{page_num}_semantic_{i}\",\n",
        "            'chunk_type': 'semantic',\n",
        "            'length': len(chunk)\n",
        "        })\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def load_pdf_with_advanced_chunking(pdf_dir: str, pdf_files: List[str]) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Load PDFs with advanced chunking strategies for better retrieval.\n",
        "    Returns list of chunks with metadata.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        file_path = os.path.join(pdf_dir, pdf_file)\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Warning: {pdf_file} not found.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with pdfplumber.open(file_path) as pdf:\n",
        "                for page_num, page in enumerate(pdf.pages):\n",
        "                    text = page.extract_text() or \"\"\n",
        "                    if not text.strip():\n",
        "                        continue\n",
        "\n",
        "                    # Advanced chunking strategies\n",
        "                    page_chunks = create_advanced_chunks(text, page_num, pdf_file)\n",
        "                    chunks.extend(page_chunks)\n",
        "\n",
        "            print(f\"Extracted {len([c for c in chunks if c['source'] == pdf_file])} chunks from {pdf_file}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {pdf_file}: {str(e)}\")\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pdf_dir = \".\"\n",
        "pdf_files = [\"aegon_travel.pdf\"]\n",
        "chunks = load_pdf_with_advanced_chunking(pdf_dir, pdf_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_enhanced_vector_index(chunks: List[Dict]) -> Tuple[faiss.IndexFlatL2, SentenceTransformer, List[Dict]]:\n",
        "    \"\"\"Build enhanced vector index with multiple embedding strategies\"\"\"\n",
        "\n",
        "    # Initialize embedder\n",
        "    embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    # Extract texts for embedding\n",
        "    texts = [chunk['text'] for chunk in chunks]\n",
        "\n",
        "    # Create embeddings\n",
        "    embeddings = embedder.encode(texts, convert_to_tensor=False, show_progress_bar=True)\n",
        "\n",
        "    # Create FAISS index\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(np.array(embeddings, dtype=np.float32))\n",
        "\n",
        "    print(f\"Built vector index with {len(chunks)} chunks\")\n",
        "\n",
        "    return index, embedder, chunks\n",
        "\n",
        "def rerank_for_diversity(chunks: List[Dict], k: int) -> List[Dict]:\n",
        "    \"\"\"Rerank chunks to ensure diversity in sources and content\"\"\"\n",
        "\n",
        "    # Sort by combined score (similarity + keyword)\n",
        "    for chunk in chunks:\n",
        "        chunk['combined_score'] = chunk['similarity_score'] + chunk['keyword_score'] * 0.3\n",
        "\n",
        "    chunks.sort(key=lambda x: x['combined_score'], reverse=True)\n",
        "\n",
        "    # Ensure diversity\n",
        "    selected_chunks = []\n",
        "    used_sources = set()\n",
        "    used_chunk_types = set()\n",
        "\n",
        "    for chunk in chunks:\n",
        "        if len(selected_chunks) >= k:\n",
        "            break\n",
        "\n",
        "        # Prioritize diversity in sources and chunk types\n",
        "        source_penalty = 0.1 if chunk['source'] in used_sources else 0\n",
        "        type_penalty = 0.05 if chunk['chunk_type'] in used_chunk_types else 0\n",
        "\n",
        "        chunk['final_score'] = chunk['combined_score'] - source_penalty - type_penalty\n",
        "\n",
        "        selected_chunks.append(chunk)\n",
        "        used_sources.add(chunk['source'])\n",
        "        used_chunk_types.add(chunk['chunk_type'])\n",
        "\n",
        "    return selected_chunks\n",
        "\n",
        "def retrieve_enhanced_context(query: str, index: faiss.IndexFlatL2, embedder: SentenceTransformer,\n",
        "                            chunks: List[Dict], k: int = 8) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Enhanced context retrieval with multiple strategies:\n",
        "    1. Semantic similarity search\n",
        "    2. Keyword matching\n",
        "    3. Diversity-based reranking\n",
        "    \"\"\"\n",
        "\n",
        "    # Semantic search\n",
        "    query_embedding = embedder.encode([query], convert_to_tensor=False)\n",
        "    distances, indices = index.search(np.array(query_embedding, dtype=np.float32), k * 2)\n",
        "\n",
        "    # Get candidate chunks\n",
        "    candidate_chunks = []\n",
        "    for i in indices[0]:\n",
        "        if i < len(chunks):\n",
        "            chunk = chunks[i].copy()\n",
        "            chunk['similarity_score'] = float(1 / (1 + distances[0][len(candidate_chunks)]))\n",
        "            candidate_chunks.append(chunk)\n",
        "\n",
        "    # Keyword matching boost\n",
        "    query_keywords = set(query.lower().split())\n",
        "    for chunk in candidate_chunks:\n",
        "        chunk_keywords = set(chunk['text'].lower().split())\n",
        "        keyword_overlap = len(query_keywords.intersection(chunk_keywords))\n",
        "        chunk['keyword_score'] = keyword_overlap / len(query_keywords) if query_keywords else 0\n",
        "\n",
        "    # Diversity-based reranking\n",
        "    final_chunks = rerank_for_diversity(candidate_chunks, k)\n",
        "\n",
        "    return final_chunks\n",
        "\n",
        "def retrieve_enhanced_context_for_df(df: pd.DataFrame, index, embedder, chunks, k = 5):\n",
        "    \"\"\"Retrieve context for each question in the DataFrame and store it in a new column.\"\"\"\n",
        "    for df_index, row in df.iterrows():\n",
        "        # Retrieve context\n",
        "        retrieved_chunks = retrieve_enhanced_context(row['question'], index, embedder, chunks, k)\n",
        "        # Format context as string\n",
        "        context_str = \"\\n\\n\".join([c['text'] for c in retrieved_chunks])\n",
        "        df.at[df_index, 'context'] = context_str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build index\n",
        "if 'chunks' in locals() and chunks:\n",
        "    index, embedder, chunks = build_enhanced_vector_index(chunks)\n",
        "    print(\"Index built successfully.\")\n",
        "else:\n",
        "    print(\"Chunks not found. Please run the PDF loading cell first.\")\n",
        "\n",
        "# Apply to DataFrame\n",
        "if 'df' in locals():\n",
        "    print(\"Retrieving context for DataFrame...\")\n",
        "    retrieve_enhanced_context_for_df(df, index, embedder, chunks)\n",
        "    print(\"Context retrieval complete. Added 'context' column to DataFrame.\")\n",
        "    print(df[['question', 'context']].head())\n",
        "else:\n",
        "    print(\"DataFrame 'df' not found. Please load the dataset first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \u5c06\u8fd9\u6bb5\u4ee3\u7801\u590d\u5236\u5230\u60a8\u7684notebook\u4e2d\n",
        "import re\n",
        "\n",
        "# \u589e\u5f3a\u7684JSON\u63d0\u53d6\u51fd\u6570\n",
        "def extract_json_from_response(response: str) -> str:\n",
        "    \"\"\"\u4ece\u54cd\u5e94\u4e2d\u63d0\u53d6JSON\uff0c\u5904\u7406\u5404\u79cd\u683c\u5f0f\"\"\"\n",
        "    # \u5904\u7406markdown\u4ee3\u7801\u5757\n",
        "    if '```json' in response:\n",
        "        match = re.search(r'```json\\s*\\n?(.*?)\\n?```', response, re.DOTALL)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "\n",
        "    if '```' in response:\n",
        "        match = re.search(r'```\\s*\\n?(.*?)\\n?```', response, re.DOTALL)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "\n",
        "    # \u67e5\u627eJSON\u5bf9\u8c61\n",
        "    json_pattern = r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}'\n",
        "    matches = re.findall(json_pattern, response, re.DOTALL)\n",
        "    if matches:\n",
        "        return max(matches, key=len).strip()\n",
        "\n",
        "    return response.strip()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cZ_4dDmxwLfB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4143f55"
      },
      "source": [
        "def load_evaluation_dataset(file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load evaluation dataset containing questions, ground truth answers, and retrieved contexts.\n",
        "\n",
        "    Expected format:\n",
        "    - CSV/Excel with columns: 'question', 'ground_truth', 'retrieved_context'\n",
        "    - Or JSON with same structure\n",
        "    \"\"\"\n",
        "    if file_path.endswith('.csv'):\n",
        "        df = pd.read_csv(file_path)\n",
        "    elif file_path.endswith('.xlsx'):\n",
        "        df = pd.read_excel(file_path)\n",
        "    elif file_path.endswith('.json'):\n",
        "        df = pd.read_json(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Use CSV, Excel, or JSON.\")\n",
        "\n",
        "    # Validate required columns\n",
        "    required_columns = ['question', 'ground_truth', 'retrieved_context']\n",
        "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "    if missing_columns:\n",
        "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
        "\n",
        "    print(f\"Loaded {len(df)} evaluation samples\")\n",
        "    print(f\"Sample data:\\n{df.head(2)}\")\n",
        "    return df"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response_with_context(question: str, context: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate a response using the provided context.\n",
        "    \"\"\"\n",
        "    deployment = os.getenv(\"DEPLOYMENT_NAME\")\n",
        "    if not deployment:\n",
        "        raise ValueError(\"DEPLOYMENT_NAME is missing in .env\")\n",
        "\n",
        "    prompt = f\"\"\"Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Please answer the question based on the provided context. Be concise and accurate.\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=deployment,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant that answers questions based on the provided context.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=150,\n",
        "        temperature=0.3  # Lower temperature for more consistent responses\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()"
      ],
      "metadata": {
        "id": "KL7dpa0xaq_W"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def judge_response_quality(question: str, response: str, ground_truth: str, context: str) -> Dict[str, any]:\n",
        "    \"\"\"\n",
        "    Evaluate response quality against ground truth using multiple criteria.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"You are an expert evaluator. Please evaluate the following response across multiple dimensions:\n",
        "\n",
        "Question: {question}\n",
        "Context Provided: {context}\n",
        "Generated Response: {response}\n",
        "Ground Truth Answer: {ground_truth}\n",
        "\n",
        "Please evaluate on these criteria (each on a scale of 0-1):\n",
        "1. **Accuracy**: How factually correct is the response compared to the ground truth?\n",
        "2. **Completeness**: Does the response cover all important points from the ground truth?\n",
        "3. **Relevance**: Is the response relevant to the question and uses the context appropriately?\n",
        "4. **Conciseness**: Is the response clear and concise without unnecessary information?\n",
        "\n",
        "Provide your evaluation in the following JSON format:\n",
        "{{\n",
        "    \"accuracy\": <float>,\n",
        "    \"completeness\": <float>,\n",
        "    \"relevance\": <float>,\n",
        "    \"conciseness\": <float>,\n",
        "    \"overall_score\": <float>,\n",
        "    \"explanation\": \"<brief explanation of the scores>\"\n",
        "}}\"\"\"\n",
        "\n",
        "    deployment = os.getenv(\"DEPLOYMENT_NAME\")\n",
        "    completion = client.chat.completions.create(\n",
        "        model=deployment,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=300,\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    result = completion.choices[0].message.content.strip()\n",
        "\n",
        "    try:\n",
        "        # Parse JSON response\n",
        "        # \u63d0\u53d6JSON\u5185\u5bb9\uff08\u53bb\u9664markdown\u6807\u8bb0\uff09\n",
        "      if '```json' in result:\n",
        "        json_content = result.split('```json')[1].split('```')[0].strip()\n",
        "      elif '```' in result:\n",
        "        json_content = result.split('```')[1].split('```')[0].strip()\n",
        "      else:\n",
        "        json_content = result\n",
        "\n",
        "# \u89e3\u6790JSON\n",
        "      evaluation = json.loads(json_content)\n",
        "      return evaluation\n",
        "    except json.JSONDecodeError:\n",
        "        # Fallback parsing if JSON fails\n",
        "        print(f\"Failed to parse JSON response: {result}\")\n",
        "        return {\n",
        "            \"accuracy\": 0.0,\n",
        "            \"completeness\": 0.0,\n",
        "            \"relevance\": 0.0,\n",
        "            \"conciseness\": 0.0,\n",
        "            \"overall_score\": 0.0,\n",
        "            \"explanation\": \"Failed to parse evaluation response\"\n",
        "        }\n"
      ],
      "metadata": {
        "id": "_2I06utWawDw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def judge_context_relevance(question: str, context: str) -> Dict[str, any]:\n",
        "    \"\"\"\n",
        "    Evaluate how relevant the retrieved context is to the question.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"You are an expert evaluator. Please evaluate how relevant the following context is to the question.\n",
        "\n",
        "Question: {question}\n",
        "Retrieved Context: {context}\n",
        "\n",
        "Please evaluate:\n",
        "1. **Relevance Score** (0-1): How relevant is the context to answering the question?\n",
        "2. **Coverage Score** (0-1): How well does the context cover the information needed to answer the question?\n",
        "3. **Noise Level** (0-1): How much irrelevant information is in the context? (0 = lots of noise, 1 = no noise)\n",
        "\n",
        "Provide your evaluation in the following JSON format:\n",
        "{{\n",
        "    \"relevance_score\": <float>,\n",
        "    \"coverage_score\": <float>,\n",
        "    \"noise_level\": <float>,\n",
        "    \"explanation\": \"<brief explanation>\"\n",
        "}}\"\"\"\n",
        "\n",
        "    deployment = os.getenv(\"DEPLOYMENT_NAME\")\n",
        "    completion = client.chat.completions.create(\n",
        "        model=deployment,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=200,\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    result = completion.choices[0].message.content.strip()\n",
        "\n",
        "    try:\n",
        "        json_content = extract_json_from_response(result)\n",
        "        evaluation = json.loads(json_content)\n",
        "        # Ensure all required fields exist\n",
        "        required_fields = ['relevance_score', 'coverage_score', 'noise_level', 'explanation']\n",
        "        for field in required_fields:\n",
        "            if field not in evaluation:\n",
        "                evaluation[field] = 0.0 if field != 'explanation' else 'Field missing from evaluation'\n",
        "        return evaluation\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Failed to parse JSON response: {result}\")\n",
        "        return {\n",
        "            \"relevance_score\": 0.0,\n",
        "            \"coverage_score\": 0.0,\n",
        "            \"noise_level\": 0.0,\n",
        "            \"explanation\": \"Failed to parse evaluation response\"\n",
        "        }\n",
        ""
      ],
      "metadata": {
        "id": "Q8Q8KNeFa23c"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_baseline_evaluation(df: pd.DataFrame, sample_size: int = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Run comprehensive baseline evaluation on the dataset.\n",
        "    \"\"\"\n",
        "    if sample_size:\n",
        "        df = df.sample(n=min(sample_size, len(df)), random_state=42)\n",
        "        print(f\"Evaluating {len(df)} samples...\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        print(f\"\\nProcessing {idx + 1}/{len(df)}...\")\n",
        "\n",
        "        question = row['question']\n",
        "        ground_truth = row['ground_truth']\n",
        "        context = row['retrieved_context']\n",
        "\n",
        "        # 1. Evaluate context relevance\n",
        "        context_eval = judge_context_relevance(question, context)\n",
        "\n",
        "        # 2. Generate response using context\n",
        "        generated_response = generate_response_with_context(question, context)\n",
        "\n",
        "        # 3. Evaluate response quality\n",
        "        response_eval = judge_response_quality(question, generated_response, ground_truth, context)\n",
        "\n",
        "        # Compile results\n",
        "        result = {\n",
        "            'question': question,\n",
        "            'ground_truth': ground_truth,\n",
        "            'retrieved_context': context[:200] + '...' if len(context) > 200 else context,\n",
        "            'generated_response': generated_response,\n",
        "\n",
        "            # Context evaluation metrics\n",
        "            'context_relevance': context_eval.get('relevance_score', 0.0),\n",
        "            'context_coverage': context_eval.get('coverage_score', 0.0),\n",
        "            'context_noise_level': context_eval.get('noise_level', 0.0),\n",
        "            'context_explanation': context_eval.get('explanation', 'N/A'),\n",
        "\n",
        "            # Response evaluation metrics\n",
        "            'response_accuracy': response_eval.get('accuracy', 0.0),\n",
        "            'response_completeness': response_eval.get('completeness', 0.0),\n",
        "            'response_relevance': response_eval.get('relevance', 0.0),  # \u6ce8\u610f\u8fd9\u91cc\u662f 'relevance' \u4e0d\u662f 'response_relevance'\n",
        "            'response_conciseness': response_eval.get('conciseness', 0.0),\n",
        "            'response_overall_score': response_eval.get('overall_score', 0.0),\n",
        "            'response_explanation': response_eval.get('explanation', 'N/A')\n",
        "        }\n",
        "\n",
        "        results.append(result)\n",
        "\n",
        "        # Print summary for this sample\n",
        "        print(f\"Context Relevance: {context_eval['relevance_score']:.2f}\")\n",
        "        print(f\"Response Overall Score: {response_eval['overall_score']:.2f}\")\n",
        "\n",
        "    return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "LTHkliJIa9gJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_baseline_metrics(results_df: pd.DataFrame) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculate aggregate baseline metrics from evaluation results.\n",
        "    \"\"\"\n",
        "    metrics = {\n",
        "        # Context metrics\n",
        "        'avg_context_relevance': results_df['context_relevance'].mean(),\n",
        "        'avg_context_coverage': results_df['context_coverage'].mean(),\n",
        "        'avg_context_noise_level': results_df['context_noise_level'].mean(),\n",
        "\n",
        "        # Response metrics\n",
        "        'avg_response_accuracy': results_df['response_accuracy'].mean(),\n",
        "        'avg_response_completeness': results_df['response_completeness'].mean(),\n",
        "        'avg_response_relevance': results_df['response_relevance'].mean(),\n",
        "        'avg_response_conciseness': results_df['response_conciseness'].mean(),\n",
        "        'avg_response_overall': results_df['response_overall_score'].mean(),\n",
        "\n",
        "        # Standard deviations\n",
        "        'std_context_relevance': results_df['context_relevance'].std(),\n",
        "        'std_response_overall': results_df['response_overall_score'].std(),\n",
        "\n",
        "        # Performance distribution\n",
        "        'excellent_responses': (results_df['response_overall_score'] >= 0.8).sum() / len(results_df),\n",
        "        'good_responses': ((results_df['response_overall_score'] >= 0.6) &\n",
        "                          (results_df['response_overall_score'] < 0.8)).sum() / len(results_df),\n",
        "        'poor_responses': (results_df['response_overall_score'] < 0.6).sum() / len(results_df)\n",
        "    }\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "-mNGk5z6bTwS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_evaluation_report(results_df: pd.DataFrame, metrics: Dict[str, float], output_dir: str = '.'):\n",
        "    \"\"\"\n",
        "    Generate comprehensive evaluation report.\n",
        "    \"\"\"\n",
        "    # Save detailed results\n",
        "    results_df.to_csv(f\"{output_dir}/baseline_evaluation_detailed.csv\", index=False)\n",
        "\n",
        "    # Create summary report\n",
        "    report = f\"\"\"\n",
        "# LLM-as-a-Judge Baseline Evaluation Report\n",
        "\n",
        "## Dataset Summary\n",
        "- Total samples evaluated: {len(results_df)}\n",
        "- Evaluation date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "## Context Retrieval Quality\n",
        "- Average Relevance: {metrics['avg_context_relevance']:.3f} (\u00b1{metrics['std_context_relevance']:.3f})\n",
        "- Average Coverage: {metrics['avg_context_coverage']:.3f}\n",
        "- Average Noise Level: {metrics['avg_context_noise_level']:.3f}\n",
        "\n",
        "## Response Generation Quality\n",
        "- Average Accuracy: {metrics['avg_response_accuracy']:.3f}\n",
        "- Average Completeness: {metrics['avg_response_completeness']:.3f}\n",
        "- Average Relevance: {metrics['avg_response_relevance']:.3f}\n",
        "- Average Conciseness: {metrics['avg_response_conciseness']:.3f}\n",
        "- **Overall Score: {metrics['avg_response_overall']:.3f} (\u00b1{metrics['std_response_overall']:.3f})**\n",
        "\n",
        "## Performance Distribution\n",
        "- Excellent (\u22650.8): {metrics['excellent_responses']:.1%}\n",
        "- Good (0.6-0.8): {metrics['good_responses']:.1%}\n",
        "- Poor (<0.6): {metrics['poor_responses']:.1%}\n",
        "\n",
        "## Top Performing Examples\n",
        "{results_df.nlargest(3, 'response_overall_score')[['question', 'response_overall_score', 'response_explanation']].to_string()}\n",
        "\n",
        "## Worst Performing Examples\n",
        "{results_df.nsmallest(3, 'response_overall_score')[['question', 'response_overall_score', 'response_explanation']].to_string()}\n",
        "\"\"\"\n",
        "\n",
        "    # Save report\n",
        "    with open(f\"{output_dir}/baseline_evaluation_report.txt\", 'w') as f:\n",
        "        f.write(report)\n",
        "\n",
        "    # Save metrics as JSON\n",
        "    with open(f\"{output_dir}/baseline_metrics.json\", 'w') as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "\n",
        "    print(report)\n",
        "    print(f\"\\nResults saved to {output_dir}/\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "OCm_nZfSbkgE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your evaluation dataset\n",
        "    # Replace with your actual data file path\n",
        "    dataset_path = \"asr_travel.xlsx\"  # or .xlsx, .json\n",
        "\n",
        "    try:\n",
        "        # Load data\n",
        "        df = load_evaluation_dataset(dataset_path)\n",
        "\n",
        "        # Run evaluation (you can specify sample_size for testing)\n",
        "        print(\"\\nStarting baseline evaluation...\")\n",
        "        results_df = run_baseline_evaluation(df, sample_size=None)  # Use sample_size=10 for testing\n",
        "\n",
        "        # Calculate metrics\n",
        "        print(\"\\nCalculating baseline metrics...\")\n",
        "        metrics = calculate_baseline_metrics(results_df)\n",
        "\n",
        "        # Generate report\n",
        "        print(\"\\nGenerating evaluation report...\")\n",
        "        import os\n",
        "        output_dir = 'evaluation_results'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        generate_evaluation_report(results_df, metrics, output_dir=output_dir)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during evaluation: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zrND7m3Abtzq",
        "outputId": "2b0e44a0-6635-4a76-d5c0-dd97d23b02f8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 33 evaluation samples\n",
            "Sample data:\n",
            "                                            question  \\\n",
            "0  I have a concussion, I want to cancel, is this...   \n",
            "1  There is a strike on the German railways next ...   \n",
            "\n",
            "                                        ground_truth  \\\n",
            "0  Yes, cancellation due to a serious illness suc...   \n",
            "1  Yes, you can cancel if there are strikes at th...   \n",
            "\n",
            "                                   retrieved_context  \n",
            "0  Event Restriction or exclusion 1. Death, serio...  \n",
            "1  Do you have to stay longer at your travel dest...  \n",
            "\n",
            "Starting baseline evaluation...\n",
            "\n",
            "Processing 1/33...\n",
            "Context Relevance: 0.80\n",
            "Response Overall Score: 0.93\n",
            "\n",
            "Processing 2/33...\n",
            "Context Relevance: 0.80\n",
            "Response Overall Score: 0.75\n",
            "\n",
            "Processing 3/33...\n",
            "Context Relevance: 0.90\n",
            "Response Overall Score: 0.97\n",
            "\n",
            "Processing 4/33...\n",
            "Context Relevance: 0.20\n",
            "Response Overall Score: 0.80\n",
            "\n",
            "Processing 5/33...\n",
            "Context Relevance: 1.00\n",
            "Response Overall Score: 0.97\n",
            "\n",
            "Processing 6/33...\n",
            "Context Relevance: 0.90\n",
            "Response Overall Score: 0.95\n",
            "\n",
            "Processing 7/33...\n",
            "Context Relevance: 0.30\n",
            "Response Overall Score: 0.93\n",
            "\n",
            "Processing 8/33...\n",
            "Context Relevance: 0.10\n",
            "Response Overall Score: 0.90\n",
            "\n",
            "Processing 9/33...\n",
            "Context Relevance: 0.20\n",
            "Response Overall Score: 0.95\n",
            "\n",
            "Processing 10/33...\n",
            "Context Relevance: 0.30\n",
            "Response Overall Score: 0.33\n",
            "\n",
            "Processing 11/33...\n",
            "Context Relevance: 0.80\n",
            "Response Overall Score: 1.00\n",
            "\n",
            "Processing 12/33...\n",
            "Context Relevance: 0.90\n",
            "Response Overall Score: 0.97\n",
            "\n",
            "Processing 13/33...\n",
            "Context Relevance: 1.00\n",
            "Response Overall Score: 0.95\n",
            "\n",
            "Processing 14/33...\n",
            "Context Relevance: 0.90\n",
            "Response Overall Score: 1.00\n",
            "\n",
            "Processing 15/33...\n",
            "Context Relevance: 0.90\n",
            "Response Overall Score: 0.90\n",
            "\n",
            "Processing 16/33...\n",
            "Context Relevance: 0.90\n",
            "Response Overall Score: 0.95\n",
            "\n",
            "Processing 17/33...\n",
            "Context Relevance: 1.00\n",
            "Response Overall Score: 1.00\n",
            "\n",
            "Processing 18/33...\n",
            "Context Relevance: 0.80\n",
            "Response Overall Score: 0.95\n",
            "\n",
            "Processing 19/33...\n",
            "Context Relevance: 0.20\n",
            "Response Overall Score: 0.68\n",
            "\n",
            "Processing 20/33...\n",
            "Context Relevance: 1.00\n",
            "Response Overall Score: 0.95\n",
            "\n",
            "Processing 21/33...\n",
            "Context Relevance: 0.80\n",
            "Response Overall Score: 1.00\n",
            "\n",
            "Processing 22/33...\n",
            "Context Relevance: 0.50\n",
            "Response Overall Score: 1.00\n",
            "\n",
            "Processing 23/33...\n",
            "Context Relevance: 0.70\n",
            "Response Overall Score: 0.93\n",
            "\n",
            "Processing 24/33...\n",
            "Context Relevance: 0.80\n",
            "Response Overall Score: 0.90\n",
            "\n",
            "Processing 25/33...\n",
            "Context Relevance: 0.30\n",
            "Response Overall Score: 0.68\n",
            "\n",
            "Processing 26/33...\n",
            "Context Relevance: 1.00\n",
            "Response Overall Score: 0.97\n",
            "\n",
            "Processing 27/33...\n",
            "Context Relevance: 0.70\n",
            "Response Overall Score: 0.53\n",
            "\n",
            "Processing 28/33...\n",
            "Context Relevance: 0.90\n",
            "Response Overall Score: 0.95\n",
            "\n",
            "Processing 29/33...\n",
            "Context Relevance: 0.80\n",
            "Response Overall Score: 0.93\n",
            "\n",
            "Processing 30/33...\n",
            "Context Relevance: 0.80\n",
            "Response Overall Score: 0.88\n",
            "\n",
            "Processing 31/33...\n",
            "Context Relevance: 0.80\n",
            "Response Overall Score: 0.68\n",
            "\n",
            "Processing 32/33...\n",
            "Context Relevance: 0.80\n",
            "Response Overall Score: 1.00\n",
            "\n",
            "Processing 33/33...\n",
            "Context Relevance: 1.00\n",
            "Response Overall Score: 0.95\n",
            "\n",
            "Calculating baseline metrics...\n",
            "\n",
            "Generating evaluation report...\n",
            "\n",
            "# LLM-as-a-Judge Baseline Evaluation Report\n",
            "\n",
            "## Dataset Summary\n",
            "- Total samples evaluated: 33\n",
            "- Evaluation date: 2025-07-15 08:20:07\n",
            "\n",
            "## Context Retrieval Quality\n",
            "- Average Relevance: 0.721 (\u00b10.281)\n",
            "- Average Coverage: 0.615\n",
            "- Average Noise Level: 0.876\n",
            "\n",
            "## Response Generation Quality\n",
            "- Average Accuracy: 0.864\n",
            "- Average Completeness: 0.776\n",
            "- Average Relevance: 0.948\n",
            "- Average Conciseness: 0.955\n",
            "- **Overall Score: 0.886 (\u00b10.153)**\n",
            "\n",
            "## Performance Distribution\n",
            "- Excellent (\u22650.8): 81.8%\n",
            "- Good (0.6-0.8): 12.1%\n",
            "- Poor (<0.6): 6.1%\n",
            "\n",
            "## Top Performing Examples\n",
            "                                                                                                                                       question  response_overall_score                                                                                                                                                                                                                                                                                                                                                                                    response_explanation\n",
            "10                                       broke a piece of my tooth, I had this corrected because I didn't like the look of it, is this covered?                     1.0                                                                                                                          The generated response accurately reflects the ground truth answer by stating that the correction for aesthetic reasons is not covered. It covers all important points, is relevant to the question and context, and is clear and concise without any unnecessary information.\n",
            "13  I am a self-employed person and my telephone that I bought privately but use for business purposes is irreparably damaged. Is this insured?                     1.0                                                                     The generated response accurately reflects the ground truth answer, stating that damage to the phone is not insured if used for business purposes. It covers all necessary points, is directly relevant to the question, and uses the context appropriately. The response is clear and concise without any unnecessary information.\n",
            "16                                                                              My suitcase fell, it has scratches and a dent. Is this insured?                     1.0  The generated response accurately reflects the information provided in the context and matches the ground truth answer. It correctly states that superficial damage such as scratches and dents is not insured, covering all important points. The response is relevant to the question and uses the context appropriately. Additionally, it is clear and concise without any unnecessary information.\n",
            "\n",
            "## Worst Performing Examples\n",
            "                                                             question  response_overall_score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 response_explanation\n",
            "9              I bought paracetamol at the pharmacy, is this covered?                   0.325         The generated response inaccurately states that buying paracetamol is covered, which contradicts the ground truth answer that it is not covered. Therefore, accuracy and completeness scores are both 0. The response is somewhat relevant as it discusses the conditions under which medication use might affect insurance coverage, but it fails to directly address the specific question about paracetamol coverage, hence a relevance score of 0.5. The response is concise, directly addressing the question without unnecessary information, resulting in a conciseness score of 0.8.\n",
            "26  I got a flat tire, which made me miss my flight. Is this covered?                   0.525  The generated response inaccurately suggests that the situation might be covered, which contradicts the ground truth answer that explicitly states it is not covered. The response does not fully address the ground truth, which clearly states the coverage exclusion. While the response is relevant to the question and uses the context appropriately, it introduces uncertainty by suggesting the need to check policy terms, which is not necessary according to the ground truth. The response is relatively concise but includes some unnecessary information about checking policy terms.\n",
            "18                I have been robbed of my car keys, is this insured?                   0.680                                                                                                                                                                                                    The response accurately notes that the provided context does not mention car keys specifically, but it fails to directly state the ground truth that car keys are not covered under travel insurance. It suggests checking the full policy, which is relevant advice, but does not fully answer the question as per the ground truth. The response is concise and avoids unnecessary information.\n",
            "\n",
            "\n",
            "Results saved to evaluation_results/\n"
          ]
        }
      ]
    }
  ]
}