import json
import os

file_path = 'Experiment1.ipynb'
with open(file_path, 'r', encoding='utf-8') as f:
    nb = json.load(f)

# Update first cell (imports)
nb['cells'
][
    0
]['source'
] = [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%pip install python-dotenv openai faiss-cpu sentence-transformers pdfplumber\n",
    "from typing import List, Dict, Tuple\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "import json\n",
    "import pdfplumber\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "def init_client():\n",
    "    \"\"\"Initialize Azure OpenAI client with environment variables.\"\"\"\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"AZURE_OPENAI_API_KEY environment variable not set.\")\n",
    "\n",
    "    endpoint = os.getenv(\"ENDPOINT_URL\")\n",
    "    if not endpoint:\n",
    "        raise ValueError(\"ENDPOINT_URL environment variable not set.\")\n",
    "\n",
    "    client = AzureOpenAI(\n",
    "        api_key=api_key,\n",
    "        api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "        azure_endpoint=endpoint\n",
    "    )\n",
    "    return client\n",
    "\n",
    "# Initialize client\n",
    "client = init_client()"
]

# Create new cells
new_cell_funcs = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "def create_sentence_chunks(sentences: List[str], chunk_size: int = 3, overlap: int = 1) -> List[str]:\n",
        "    \"\"\"Create overlapping sentence chunks\"\"\"\n",
        "    chunks = []\n",
        "    for i in range(0, len(sentences), chunk_size - overlap):\n",
        "        chunk_sentences = sentences[i:i + chunk_size]\n",
        "        if chunk_sentences:\n",
        "            chunks.append(' '.join(chunk_sentences))\n",
        "    return chunks\n",
        "\n",
        "def create_paragraph_chunks(paragraphs: List[str], max_length: int = 800, overlap: int = 100) -> List[str]:\n",
        "    \"\"\"Create overlapping paragraph chunks\"\"\"\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for para in paragraphs:\n",
        "        if len(current_chunk) + len(para) <= max_length:\n",
        "            current_chunk += para + \"\\n\\n\"\n",
        "        else:\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "                # Create overlap\n",
        "                current_chunk = current_chunk[-overlap:] + para + \"\\n\\n\"\n",
        "            else:\n",
        "                current_chunk = para + \"\\n\\n\"\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def create_semantic_chunks(text: str) -> List[str]:\n",
        "    \"\"\"Create chunks based on semantic sections (headers, numbered sections, etc.)\"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    # Pattern for detecting sections/headers\n",
        "    section_pattern = r'(?:^|\\n)(?:\\d+\\.?\\s+|[A-Z][^.]*:|\\n[A-Z][A-Z\\s]+\\n|Chapter\\s+\\d+|Section\\s+\\d+)'\n",
        "\n",
        "    sections = re.split(section_pattern, text)\n",
        "\n",
        "    for section in sections:\n",
        "        section = section.strip()\n",
        "        if len(section) > 100:  # Only keep substantial sections\n",
        "            chunks.append(section)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def create_advanced_chunks(text: str, page_num: int, source: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Create advanced chunks using multiple strategies:\n",
        "    1. Sentence-based chunking\n",
        "    2. Paragraph-based chunking\n",
        "    3. Overlapping chunks\n",
        "    4. Semantic chunks based on headers/sections\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    # Clean text\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Strategy 1: Sentence-based chunking\n",
        "    sentences = sent_tokenize(text)\n",
        "    sentence_chunks = create_sentence_chunks(sentences, chunk_size=3, overlap=1)\n",
        "\n",
        "    for i, chunk in enumerate(sentence_chunks):\n",
        "        chunks.append({\n",
        "            'text': chunk,\n",
        "            'source': source,\n",
        "            'page': page_num,\n",
        "            'chunk_id': f\"{source}_p{page_num}_sent_{i}\",\n",
        "            'chunk_type': 'sentence',\n",
        "            'length': len(chunk)\n",
        "        })\n",
        "\n",
        "    # Strategy 2: Paragraph-based chunking\n",
        "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "    paragraph_chunks = create_paragraph_chunks(paragraphs, max_length=800, overlap=100)\n",
        "\n",
        "    for i, chunk in enumerate(paragraph_chunks):\n",
        "        chunks.append({\n",
        "            'text': chunk,\n",
        "            'source': source,\n",
        "            'page': page_num,\n",
        "            'chunk_id': f\"{source}_p{page_num}_para_{i}\",\n",
        "            'chunk_type': 'paragraph',\n",
        "            'length': len(chunk)\n",
        "        })\n",
        "\n",
        "    # Strategy 3: Semantic chunking (based on headers/sections)\n",
        "    semantic_chunks = create_semantic_chunks(text)\n",
        "\n",
        "    for i, chunk in enumerate(semantic_chunks):\n",
        "        chunks.append({\n",
        "            'text': chunk,\n",
        "            'source': source,\n",
        "            'page': page_num,\n",
        "            'chunk_id': f\"{source}_p{page_num}_semantic_{i}\",\n",
        "            'chunk_type': 'semantic',\n",
        "            'length': len(chunk)\n",
        "        })\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def load_pdf_with_advanced_chunking(pdf_dir: str, pdf_files: List[str]) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Load PDFs with advanced chunking strategies for better retrieval.\n",
        "    Returns list of chunks with metadata.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        file_path = os.path.join(pdf_dir, pdf_file)\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Warning: {pdf_file} not found.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with pdfplumber.open(file_path) as pdf:\n",
        "                for page_num, page in enumerate(pdf.pages):\n",
        "                    text = page.extract_text() or \"\"\n",
        "                    if not text.strip():\n",
        "                        continue\n",
        "\n",
        "                    # Advanced chunking strategies\n",
        "                    page_chunks = create_advanced_chunks(text, page_num, pdf_file)\n",
        "                    chunks.extend(page_chunks)\n",
        "\n",
        "            print(f\"Extracted {len([c for c in chunks if c['source'] == pdf_file])} chunks from {pdf_file}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {pdf_file}: {str(e)}\")\n",
        "\n",
        "    return chunks"
    ]
}

new_cell_load = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "pdf_dir = \".\"\n",
        "pdf_files = [\"aegon_travel.pdf\"]\n",
        "chunks = load_pdf_with_advanced_chunking(pdf_dir, pdf_files)"
    ]
}

# Insert new cells after the first one
nb['cells'
].insert(1, new_cell_funcs)
nb['cells'
].insert(2, new_cell_load)

with open(file_path, 'w', encoding='utf-8') as f:
    json.dump(nb, f, indent=2)

print("Successfully modified Experiment1.ipynb")
